# -*- coding: utf-8 -*-
"""
Multi-Agent Orchestrator Module
ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
"""

import os
import json
import asyncio
import time
from typing import Optional

from llm_client import LLMClient
from agents import QuestionUnderstandingAgent, ToolBasedAgent
from mcp_manager import MCPManager


class MultiAgentOrchestrator:
    """ë©€í‹° ì—ì´ì „íŠ¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°"""

    def __init__(self):
        self.mcp_manager = MCPManager()
        self.llm_client: Optional[LLMClient] = None
        self.question_agent: Optional[QuestionUnderstandingAgent] = None
        self.specialist_agents = {}

    async def connect_mcp_server(self, server_name: str, base_url: str, auth_bearer: str = ""):
        """MCP ì„œë²„ ì—°ê²°"""
        return await self.mcp_manager.connect_mcp_server(server_name, base_url, auth_bearer)

    def initialize_agents(self):
        """ì—ì´ì „íŠ¸ ì´ˆê¸°í™”"""
        # LLM ì œê³µì ì„ íƒ
        llm_provider = os.environ.get("LLM_PROVIDER", "azure").lower()

        # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        if llm_provider == "azure":
            self.llm_client = LLMClient(
                provider="azure",
                api_key=os.environ["AZURE_OPENAI_API_KEY"],
                api_version=os.environ["AZURE_OPENAI_API_VERSION"],
                azure_endpoint=os.environ["AZURE_OPENAI_ENDPOINT"],
                deployment=os.environ["AZURE_OPENAI_DEPLOYMENT"],
            )
        elif llm_provider == "openai":
            self.llm_client = LLMClient(
                provider="openai",
                api_key=os.environ.get("OPENAI_API_KEY"),
                base_url=os.environ.get("OPENAI_BASE_URL"),
                model=os.environ.get("OPENAI_MODEL", "gpt-4"),
            )
        elif llm_provider == "vllm":
            self.llm_client = LLMClient(
                provider="vllm",
                api_key=os.environ.get("VLLM_API_KEY", "EMPTY"),
                base_url=os.environ["VLLM_BASE_URL"],
                model=os.environ.get("VLLM_MODEL", "meta-llama/Llama-2-7b-chat-hf"),
            )
        elif llm_provider == "google":
            self.llm_client = LLMClient(
                provider="google",
                api_key=os.environ.get("GEMINI_API_KEY"),
                base_url=os.environ.get("GEMINI_BASE_URL"),
                model=os.environ.get("GEMINI_MODEL", "gemini-1.5-pro"),
            )
        elif llm_provider == "anthropic":
            self.llm_client = LLMClient(
                provider="anthropic",
                api_key=os.environ.get("ANTHROPIC_API_KEY"),
                base_url=os.environ.get("ANTHROPIC_BASE_URL"),
                model=os.environ.get("ANTHROPIC_MODEL", "claude-3-5-sonnet-20241022"),
            )
        elif llm_provider == "xai":
            self.llm_client = LLMClient(
                provider="xai",
                api_key=os.environ.get("XAI_API_KEY"),
                base_url=os.environ.get("XAI_BASE_URL"),
                model=os.environ.get("XAI_MODEL", "grok-beta"),
            )
        else:
            raise ValueError(f"Unsupported LLM_PROVIDER: {llm_provider}")

        # ê° ì„œë²„ë³„ ë„êµ¬ ì •ë³´ ìˆ˜ì§‘
        agent_tools_info = {}
        for server_name in self.mcp_manager.servers.keys():
            server_tools = [tool for tool in self.mcp_manager.all_tools
                          if getattr(tool, 'name', '').startswith(f'{server_name}__')]
            tool_names = [getattr(tool, 'name', '').replace(f'{server_name}__', '')
                         for tool in server_tools]
            agent_tools_info[server_name] = tool_names

        # Agent A ì´ˆê¸°í™”
        available_agents = list(self.mcp_manager.servers.keys())
        self.question_agent = QuestionUnderstandingAgent(
            self.llm_client, available_agents, agent_tools_info
        )

        # ì „ë¬¸ ì—ì´ì „íŠ¸ ìƒì„±
        print(f"\nâœ… ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ:")
        print(f"   â€¢ {self.question_agent.name}: {self.question_agent.role}")

        for server_name in self.mcp_manager.servers.keys():
            server_tools = [tool for tool in self.mcp_manager.all_tools
                          if getattr(tool, 'name', '').startswith(f'{server_name}__')]

            agent = ToolBasedAgent(
                name=f"{server_name.upper()}Agent",
                role=f"{server_name} ì „ë¬¸ ì„œë¹„ìŠ¤",
                llm_client=self.llm_client,
                tools=server_tools
            )
            self.specialist_agents[server_name] = agent
            print(f"   â€¢ {agent.name}: {agent.role} (ë„êµ¬ {len(server_tools)}ê°œ)")

    async def close_all_servers(self):
        """ëª¨ë“  MCP ì„œë²„ ì—°ê²° ì¢…ë£Œ"""
        await self.mcp_manager.close_all_servers()
        self.specialist_agents.clear()


async def run_multi_agent_conversation(orchestrator: MultiAgentOrchestrator, user_query: str) -> str:
    """ë©€í‹° ì—ì´ì „íŠ¸ ëŒ€í™” ì‹¤í–‰"""
    print(f"\n{'='*70}")
    print(f"ğŸ¤– Multi-Agent Processing Pipeline")
    print(f"{'='*70}\n")

    # Step 1: Agent A - ì§ˆë¬¸ ë¶„ì„
    print(f"ğŸ“‹ [Step 1] Agent A: ì§ˆë¬¸ ë¶„ì„ ë° ë¼ìš°íŒ…")
    print(f"{'â”€'*70}")

    step1_start = time.time()
    question_response = await orchestrator.question_agent.process(user_query)
    step1_time = time.time() - step1_start

    if not question_response.success:
        print(f"âŒ ì§ˆë¬¸ ë¶„ì„ ì‹¤íŒ¨: {question_response.content}")
        return question_response.content

    print(f"âœ… ë¶„ì„ ì™„ë£Œ ({step1_time:.2f}ì´ˆ)")
    print(f"\n{question_response.content}\n")

    # JSON íŒŒì‹±
    try:
        content = question_response.content
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].split("```")[0].strip()

        analysis = json.loads(content)
        execution_order = analysis.get("execution_order", [])
        question_type = analysis.get("question_type", "general")
        queries = analysis.get("queries", {})
        dependencies = analysis.get("dependencies", {})

        print(f"ğŸ¯ íŒë‹¨ ê²°ê³¼:")
        print(f"   ì§ˆë¬¸ ìœ í˜•: {question_type}")

        if execution_order:
            execution_plan_str = " â†’ ".join([
                f"({', '.join(group)})" if len(group) > 1 else group[0]
                for group in execution_order
            ])
            print(f"   ì‹¤í–‰ ìˆœì„œ: {execution_plan_str}")
        else:
            print(f"   ì‹¤í–‰ ìˆœì„œ: none")

        if queries:
            flat_order = [agent for group in execution_order for agent in group]
            for agent in flat_order:
                if agent in queries:
                    print(f"   - {agent}: {queries[agent]}")
                    if agent in dependencies:
                        print(f"      â””â”€ ì˜ì¡´ì„±: {dependencies[agent]}")
        print()

    except json.JSONDecodeError:
        print(f"âš ï¸  JSON íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ ì²˜ë¦¬ë¡œ ì§„í–‰\n")
        execution_order = []
        queries = {}
        dependencies = {}

    # Step 2: ì „ë¬¸ ì—ì´ì „íŠ¸ë“¤ ì‹¤í–‰
    if not execution_order:
        # ì¼ë°˜ ì§ˆë¬¸ - Agent Aê°€ ì§ì ‘ ë‹µë³€
        print(f"ğŸ’¬ [Final Answer] Agent A ì§ì ‘ ë‹µë³€")
        print(f"{'â”€'*70}\n")

        try:
            messages = [
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
                {"role": "user", "content": user_query}
            ]

            stream_start = time.time()
            stream_response = orchestrator.llm_client.chat_completion(messages, stream=True)

            collected_content = ""
            for chunk in stream_response:
                if chunk.choices and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta
                    if hasattr(delta, 'content') and delta.content:
                        print(delta.content, end="", flush=True)
                        collected_content += delta.content

            stream_time = time.time() - stream_start
            print(f"\n\nâ±ï¸  ë‹µë³€ ìƒì„± ì‹œê°„: {stream_time:.2f}ì´ˆ")

            return collected_content

        except Exception as e:
            print(f"\nâš ï¸  ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return question_response.content

    agent_results = {}
    previous_results = []
    step_num = 2

    # execution_orderë¥¼ ê·¸ë£¹ë³„ë¡œ ì‹¤í–‰
    for group_idx, agent_group in enumerate(execution_order):
        if not agent_group:
            continue

        # ë‹¨ì¼ ì—ì´ì „íŠ¸ (ìˆœì°¨ ì‹¤í–‰)
        if len(agent_group) == 1:
            agent_name = agent_group[0]
            if agent_name not in orchestrator.specialist_agents:
                print(f"âš ï¸  ì—ì´ì „íŠ¸ '{agent_name}' not found, skipping...")
                continue

            specialist_agent = orchestrator.specialist_agents[agent_name]
            print(f"ğŸ”§ [Step {step_num}] {specialist_agent.name} ì²˜ë¦¬ (ìˆœì°¨)")
            print(f"{'â”€'*70}")

            query = queries.get(agent_name, user_query)
            dependency = dependencies.get(agent_name, "")

            print(f"ì§ˆë¬¸: {query}")
            if dependency and previous_results:
                print(f"ì˜ì¡´ì„±: {dependency}")
            print()

            step_start = time.time()
            context = {
                "original_query": user_query,
                "analysis": question_response.content,
                "structured_query": query,
            }
            if previous_results:
                context["previous_agent_results"] = previous_results
                if dependency:
                    context["dependency_instruction"] = dependency

            response = await specialist_agent.process_with_tools(query, context)
            step_time = time.time() - step_start

            if response.success:
                print(f"\nâœ… ì²˜ë¦¬ ì™„ë£Œ ({step_time:.2f}ì´ˆ)\n")
                result_info = {
                    "agent": specialist_agent.name,
                    "agent_name": agent_name,
                    "query": query,
                    "response": response.content,
                    "time": step_time
                }
                agent_results[agent_name] = result_info
                previous_results.append(result_info)
            else:
                print(f"\nâŒ ì²˜ë¦¬ ì‹¤íŒ¨: {response.content}\n")

        # ì—¬ëŸ¬ ì—ì´ì „íŠ¸ (ë³‘ë ¬ ì‹¤í–‰)
        else:
            print(f"ğŸš€ [Step {step_num}] {len(agent_group)}ê°œ ì—ì´ì „íŠ¸ ë³‘ë ¬ ì²˜ë¦¬: {', '.join(agent_group)}")
            print(f"{'â”€'*70}")

            tasks = []
            task_agent_names = []

            for agent_name in agent_group:
                if agent_name not in orchestrator.specialist_agents:
                    continue

                specialist_agent = orchestrator.specialist_agents[agent_name]
                query = queries.get(agent_name, user_query)

                context = {
                    "original_query": user_query,
                    "structured_query": query,
                }
                if previous_results:
                    context["previous_agent_results"] = previous_results

                tasks.append(specialist_agent.process_with_tools(query, context))
                task_agent_names.append(agent_name)

            if tasks:
                step_start = time.time()
                parallel_responses = await asyncio.gather(*tasks)
                step_time = time.time() - step_start

                print(f"âœ… ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ ({step_time:.2f}ì´ˆ)\n")

                for agent_name, response in zip(task_agent_names, parallel_responses):
                    if response.success:
                        result_info = {
                            "agent": response.agent_name,
                            "agent_name": agent_name,
                            "query": queries.get(agent_name, user_query),
                            "response": response.content,
                            "time": step_time
                        }
                        agent_results[agent_name] = result_info
                        previous_results.append(result_info)

        step_num += 1

    if not agent_results:
        return question_response.content

    # Step 3: Agent A - ê²°ê³¼ í†µí•©
    print(f"ğŸ”„ [Step {step_num}] Agent A: ê²°ê³¼ í†µí•© ë° ìµœì¢… ë‹µë³€ ìƒì„±")
    print(f"{'â”€'*70}\n")

    try:
        expert_answers = ""
        flat_execution_order = [agent for group in execution_order for agent in group]
        for i, agent_name in enumerate(flat_execution_order, 1):
            if agent_name in agent_results:
                result = agent_results[agent_name]
                expert_answers += f"\n\n[{i}ë‹¨ê³„: {result['agent']}ì˜ ë‹µë³€]\nì§ˆë¬¸: {result['query']}\në‹µë³€: {result['response']}"

        messages = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì—¬ëŸ¬ ì „ë¬¸ê°€ì˜ ë‹µë³€ì„ í†µí•©í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ì œê³µí•˜ëŠ” ì½”ë””ë„¤ì´í„°ì…ë‹ˆë‹¤."},
            {"role": "user", "content": f"ì›ë³¸ ì§ˆë¬¸: {user_query}\n\nì „ë¬¸ê°€ ë‹µë³€ë“¤:{expert_answers}\n\nìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”."}
        ]

        stream_response = orchestrator.llm_client.chat_completion(messages, stream=True)

        collected_content = ""
        for chunk in stream_response:
            if chunk.choices and len(chunk.choices) > 0:
                delta = chunk.choices[0].delta
                if hasattr(delta, 'content') and delta.content:
                    print(delta.content, end="", flush=True)
                    collected_content += delta.content

        print(f"\n")
        return collected_content

    except Exception as e:
        print(f"\nâš ï¸  í†µí•© ì‹¤íŒ¨: {e}")
        return "\n\n".join([f"[{r['agent']}]\n{r['response']}" for r in agent_results.values()])

