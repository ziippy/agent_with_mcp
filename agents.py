# -*- coding: utf-8 -*-
"""
Agent Classes Module
ë‹¤ì–‘í•œ ì—ì´ì „íŠ¸ í´ë˜ìŠ¤ ì •ì˜
"""

import os
import json
from typing import Any, Dict, List, Optional

from llm_client import LLMClient, ContentFilterError
from google.adk.tools import BaseTool


class AgentResponse:
    """ì—ì´ì „íŠ¸ ì‘ë‹µ"""
    def __init__(self, content: str, metadata: Dict[str, Any], success: bool, agent_name: str):
        self.content = content
        self.metadata = metadata
        self.success = success
        self.agent_name = agent_name


class SpecializedAgent:
    """íŠ¹í™”ëœ ì—ì´ì „íŠ¸ ê¸°ë³¸ í´ë˜ìŠ¤"""

    def __init__(self, name: str, role: str, system_prompt: str, llm_client: LLMClient):
        self.name = name
        self.role = role
        self.system_prompt = system_prompt
        self.llm_client = llm_client

    async def process(self, user_input: str, context: Optional[Dict[str, Any]] = None) -> AgentResponse:
        """ì—ì´ì „íŠ¸ ì²˜ë¦¬"""
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": user_input}
        ]

        if context:
            context_str = f"\n\n[Context from previous agents]\n{json.dumps(context, indent=2, ensure_ascii=False)}"
            messages[-1]["content"] += context_str

        try:
            response = self.llm_client.chat_completion(messages, stream=False)
            content = response.choices[0].message.content or ""

            return AgentResponse(
                content=content,
                metadata={"tokens": response.usage.total_tokens if hasattr(response, 'usage') else 0},
                success=True,
                agent_name=self.name
            )
        except ContentFilterError as e:
            return AgentResponse(
                content=f"ì½˜í…ì¸  í•„í„°ë§ ì°¨ë‹¨: {', '.join(e.filtered_categories)}",
                metadata={"error": str(e)},
                success=False,
                agent_name=self.name
            )
        except Exception as e:
            return AgentResponse(
                content=f"ì—ëŸ¬ ë°œìƒ: {str(e)}",
                metadata={"error": str(e)},
                success=False,
                agent_name=self.name
            )


class QuestionUnderstandingAgent(SpecializedAgent):
    """Agent A: ì§ˆë¬¸ ì´í•´ ë° ë¼ìš°íŒ… ë‹´ë‹¹ (A2A 1ë‹¨ê³„)"""

    def __init__(self, llm_client: LLMClient, available_agents: List[str], agent_descriptions: Dict[str, str]):
        """
        Args:
            llm_client: LLM í´ë¼ì´ì–¸íŠ¸
            available_agents: ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸ ëª©ë¡
            agent_descriptions: ê° ì—ì´ì „íŠ¸ì˜ ì—­í•  ì„¤ëª… {"laws": "ë²•ë¥  ê²€ìƒ‰ ì „ë¬¸...", ...}
        """
        agents_str = ", ".join(available_agents)

        # ê° ì—ì´ì „íŠ¸ì˜ ì—­í•  ì„¤ëª…ì„ ë¬¸ìì—´ë¡œ í¬ë§·
        agents_info_str = ""
        for agent, description in agent_descriptions.items():
            agents_info_str += f"\n  - **{agent}**: {description}"

        system_prompt = f"""ë‹¹ì‹ ì€ Multi-Agent ì‹œìŠ¤í…œì˜ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ì „ë¬¸ ì—ì´ì „íŠ¸ì—ê²Œ ì‘ì—…ì„ ìœ„ì„í•©ë‹ˆë‹¤.

**ğŸ¤– ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸:**{agents_info_str}

**âš ï¸ A2A (Agent-to-Agent) 1ë‹¨ê³„ ì›ì¹™:**
1. ë‹¹ì‹ ì€ **ì—ì´ì „íŠ¸ì˜ ì—­í• ê³¼ ì„¤ëª…ë§Œ** ë³´ê³  ë¼ìš°íŒ…í•©ë‹ˆë‹¤
2. ê° ì—ì´ì „íŠ¸ê°€ **ì–´ë–¤ ë„êµ¬ë¥¼ ê°€ì§€ê³  ìˆëŠ”ì§€ëŠ” ì•Œ í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤**
3. ê° ì—ì´ì „íŠ¸ëŠ” **ììœ¨ì ìœ¼ë¡œ** ìì‹ ì˜ ë„êµ¬ë¥¼ ì„ íƒí•©ë‹ˆë‹¤

**ğŸ¯ í•µì‹¬ ê·œì¹™: ê°€ëŠ¥í•˜ë©´ ë³‘ë ¬ ì‹¤í–‰í•˜ì„¸ìš”!**

**ì‹¤í–‰ ì „ëµ ê²°ì • ê°€ì´ë“œ:**

ğŸ”€ **ë³‘ë ¬ ì‹¤í–‰ (PARALLEL) - ìš°ì„  ê³ ë ¤!**
- í˜•ì‹: execution_order: [["agent1", "agent2"]]
- ì¡°ê±´: ë‘ ì—ì´ì „íŠ¸ì˜ ì‘ì—…ì´ **ì„œë¡œ ë…ë¦½ì **ì¼ ë•Œ
- íŒë‹¨: agent2ê°€ agent1ì˜ ê²°ê³¼ë¥¼ **ê¼­ ë´ì•¼ í•˜ë‚˜?** â†’ NOë©´ ë³‘ë ¬!
- ì˜ˆì‹œ:
  * "12ëŒ€ ì¤‘ê³¼ì‹¤ì´ ë­ì•¼?" â†’ [["laws", "search"]] âœ…
  * "ê·¼ë¡œê¸°ì¤€ë²• ì„¤ëª…í•˜ê³  ê´€ë ¨ ë‰´ìŠ¤ ì°¾ì•„ì¤˜" â†’ [["laws", "search"]] âœ…
  * "ë²•ë¥  ì •ì˜ì™€ íŒë¡€ ì•Œë ¤ì¤˜" â†’ [["laws", "precedent"]] âœ…

â­ï¸ **ìˆœì°¨ ì‹¤í–‰ (SEQUENTIAL) - ì˜ì¡´ì„±ì´ ëª…í™•í•  ë•Œë§Œ!**
- í˜•ì‹: execution_order: [["agent1"], ["agent2"]]
- ì¡°ê±´: agent2ê°€ agent1ì˜ **êµ¬ì²´ì  ê²°ê³¼ë¥¼ ë°˜ë“œì‹œ í•„ìš”**ë¡œ í•  ë•Œ
- íŒë‹¨: "~ë¥¼ ì°¾ê³ ", "~í•œ í›„", "~ë¥¼ ë°”íƒ•ìœ¼ë¡œ" ê°™ì€ **ëª…ì‹œì  ìˆœì„œ**ê°€ ìˆì„ ë•Œ
- **ì¤‘ìš”:** agent2ì˜ queryëŠ” "ì´ì „ ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì—¬..." í˜•íƒœë¡œ ì‘ì„±
  * âŒ "ì•ì„œ ì°¾ì€ ìœ„ë°˜ ì‚¬ë¡€ì— í•´ë‹¹ë˜ëŠ” ë²• ì¡°í•­..." (ëª¨í˜¸í•¨)
  * âœ… "ì´ì „ ì—ì´ì „íŠ¸ê°€ ì°¾ì€ ê·¼ë¡œê¸°ì¤€ë²• ìœ„ë°˜ ì‚¬ë¡€ë“¤ì„ ë¶„ì„í•˜ì—¬, ê° ì‚¬ë¡€ì— í•´ë‹¹í•˜ëŠ” ë²• ì¡°í•­ì„ ì°¾ì•„ì£¼ì„¸ìš”" (ëª…í™•í•¨)
- ì˜ˆì‹œ:
  * "ìµœê·¼ íŒë¡€ë¥¼ ì°¾ê³ , ê·¸ íŒë¡€ì˜ ë²•ë¥  ì¡°í•­ì„ ì•Œë ¤ì¤˜" â†’ [["precedent"], ["laws"]] âœ…
    queries: {{
      "precedent": "ìµœê·¼ ê·¼ë¡œê¸°ì¤€ë²• ê´€ë ¨ íŒë¡€ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”",
      "laws": "ì´ì „ì— ì°¾ì€ íŒë¡€ì—ì„œ ì–¸ê¸‰ëœ ë²•ë¥  ì¡°í•­ë“¤ì„ ìƒì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”"
    }}
  * "ì´ ì‚¬ê±´ì˜ ê´€ë ¨ ë²•ë¥ ì„ ë¨¼ì € ì°¾ê³ , ê·¸ ë²•ë¥ ì˜ ìœ„ë°˜ ì‚¬ë¡€ë¥¼ ì°¾ì•„ì¤˜" â†’ [["laws"], ["search"]] âœ…
    queries: {{
      "laws": "êµí†µì‚¬ê³  ê´€ë ¨ ë²•ë¥  ì¡°í•­ì„ ì°¾ì•„ì£¼ì„¸ìš”",
      "search": "ì´ì „ì— ì°¾ì€ ë²•ë¥  ì¡°í•­ë“¤ì˜ ìœ„ë°˜ ì‚¬ë¡€ë¥¼ ê²€ìƒ‰í•´ì£¼ì„¸ìš”"
    }}

ğŸ’¬ **ë‹¨ì¼ ì‹¤í–‰ (SINGLE)**
- í˜•ì‹: execution_order: [["agent1"]]
- ì¡°ê±´: í•œ ì—ì´ì „íŠ¸ë§Œ ëª…í™•íˆ í•„ìš”í•  ë•Œ

âŒ **ì¼ë°˜ ëŒ€í™”**
- í˜•ì‹: execution_order: []
- ì¡°ê±´: ì „ë¬¸ ì§€ì‹ì´ í•„ìš” ì—†ëŠ” ì¼ë°˜ ëŒ€í™”

**âš ï¸ ë§¤ìš° ì¤‘ìš” - execution_order ì‘ì„± ê·œì¹™:**
1. execution_orderì—ëŠ” **ë°˜ë“œì‹œ ì—ì´ì „íŠ¸ ì´ë¦„ë§Œ** ì‚¬ìš©
2. ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸ ì´ë¦„: {agents_str}

**ì‘ë‹µ í˜•ì‹ (JSON):**
{{
  "question_type": "parallel|sequential|single|general",
  "execution_order": [["agent1", "agent2"]] or [["agent1"], ["agent2"]] or [["agent1"]] or [],
  "queries": {{
    "agent1": "êµ¬ì²´ì ì¸ ì§ˆë¬¸",
    "agent2": "êµ¬ì²´ì ì¸ ì§ˆë¬¸"
  }},
  "dependencies": {{
    "agent_name": "ì˜ì¡´ì„± ì„¤ëª… (ì„ íƒì‚¬í•­)"
  }},
  "analysis": "ì§ˆë¬¸ ë¶„ì„ ë° ì‹¤í–‰ ìˆœì„œ ê²°ì • ì´ìœ "
}}"""

        super().__init__(
            name="QuestionUnderstandingAgent",
            role="ì§ˆë¬¸ ì´í•´ ë° ë¼ìš°íŒ… (A2A 1ë‹¨ê³„)",
            system_prompt=system_prompt,
            llm_client=llm_client
        )


class ToolBasedAgent(SpecializedAgent):
    """ë„êµ¬ ê¸°ë°˜ ì „ë¬¸ ì—ì´ì „íŠ¸ (A2A Phase 2 ì§€ì›)"""

    def __init__(self, name: str, role: str, llm_client: LLMClient, tools: List[BaseTool],
                 description: str = "", orchestrator=None):
        # ë„êµ¬ ì •ë³´ ìˆ˜ì§‘
        tools_info = []
        for tool in tools:
            tool_name = getattr(tool, 'name', type(tool).__name__)
            tool_input_schema = getattr(tool, 'input_schema', None)
            if tool_input_schema and 'properties' in tool_input_schema:
                params = list(tool_input_schema['properties'].keys())
                required = tool_input_schema.get('required', [])
                params_str = ', '.join([f"{p}{'*' if p in required else ''}" for p in params])
                tools_info.append(f"  - {tool_name}({params_str})")

        tools_detail = "\n".join(tools_info) if tools_info else "(ë„êµ¬ ì •ë³´ ì—†ìŒ)"

        system_prompt = f"""ë‹¹ì‹ ì€ {role} ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ì—­í• :** {description}

**A2A Phase 2: ì—ì´ì „íŠ¸ ê°„ í˜‘ë ¥**
ë‹¹ì‹ ì€ ììœ¨ì ìœ¼ë¡œ ë‹¤ìŒì„ ê²°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
1. **ë„êµ¬ ì‚¬ìš©**: ìì‹ ì´ ê°€ì§„ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ì ‘ ì‘ì—… ìˆ˜í–‰
2. **ì—ì´ì „íŠ¸ í˜‘ë ¥**: ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì˜ ë„ì›€ì´ í•„ìš”í•˜ë©´ request_agent_help ë„êµ¬ë¥¼ ì‚¬ìš©

**ì¤‘ìš”: ë„êµ¬ ì‚¬ìš© ì‹œ ë°˜ë“œì‹œ ì•„ë˜ì˜ ì •í™•í•œ íŒŒë¼ë¯¸í„° ì´ë¦„ì„ ì‚¬ìš©í•˜ì„¸ìš”!**

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬: {len(tools)}ê°œ
{tools_detail}
(*í‘œì‹œëŠ” í•„ìˆ˜ íŒŒë¼ë¯¸í„°)

**í˜‘ë ¥ ì›ì¹™:**
- ìì‹ ì˜ ì „ë¬¸ ì˜ì—­ ë‚´ ì‘ì—…ì€ ìì‹ ì˜ ë„êµ¬ë¡œ í•´ê²°
- ë‹¤ë¥¸ ì „ë¬¸ ì˜ì—­ì˜ ì‘ì—…ì´ í•„ìš”í•˜ë©´ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì—ê²Œ í˜‘ë ¥ ìš”ì²­
- request_agent_help ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì—ê²Œ ì‘ì—… ìœ„ì„

**ë„êµ¬ íŒŒë¼ë¯¸í„° ê·œì¹™:**
1. ë„êµ¬ í˜¸ì¶œ ì‹œ ìŠ¤í‚¤ë§ˆì— ì •ì˜ëœ **ì •í™•í•œ íŒŒë¼ë¯¸í„° ì´ë¦„**ì„ ì‚¬ìš©í•˜ì„¸ìš”
2. 'keyword' ëŒ€ì‹  'query', 'search_text' ë“± ì‹¤ì œ ì •ì˜ëœ ì´ë¦„ ì‚¬ìš©
3. í•„ìˆ˜ íŒŒë¼ë¯¸í„°(*)ëŠ” ë°˜ë“œì‹œ í¬í•¨"""

        super().__init__(
            name=name,
            role=role,
            system_prompt=system_prompt,
            llm_client=llm_client
        )
        self.tools = tools
        self.description = description
        self.orchestrator = orchestrator  # A2A Phase 2ë¥¼ ìœ„í•œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì°¸ì¡°

    def set_orchestrator(self, orchestrator):
        """ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì„¤ì • (A2A Phase 2)"""
        self.orchestrator = orchestrator

    async def process_with_tools(self, user_input: str, context: Optional[Dict[str, Any]] = None):
        """ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì²˜ë¦¬ (A2A Phase 2: ì—ì´ì „íŠ¸ ê°„ í˜‘ë ¥ ì§€ì›)"""
        tools_for_openai = []

        print(f"\n  ğŸ“‹ [{self.name}] ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ Schema:", flush=True)

        for tool in self.tools:
            tool_name = getattr(tool, 'name', type(tool).__name__)
            tool_description = getattr(tool, 'description', '')
            tool_input_schema = getattr(tool, 'input_schema', None) or {"type": "object", "properties": {}}

            tools_for_openai.append({
                "type": "function",
                "function": {
                    "name": tool_name,
                    "description": tool_description or "",
                    "parameters": tool_input_schema,
                },
            })
        
        # ğŸ¤ A2A Phase 2: ë‹¤ë¥¸ ì—ì´ì „íŠ¸ í˜‘ë ¥ ìš”ì²­ ë„êµ¬ ì¶”ê°€
        if self.orchestrator:
            available_agents = [name for name in self.orchestrator.specialist_agents.keys()
                              if name != self.name.replace("Agent", "").lower()]
            
            if available_agents:
                # ê° ì—ì´ì „íŠ¸ì˜ ì„¤ëª… ìˆ˜ì§‘
                agents_info = []
                for agent_name in available_agents:
                    agent = self.orchestrator.specialist_agents.get(agent_name)
                    if agent and hasattr(agent, 'description'):
                        agents_info.append(f"  - {agent_name}: {agent.description}")
                
                agents_info_str = "\n".join(agents_info) if agents_info else "\n".join([f"  - {name}" for name in available_agents])
                
                # ì—ì´ì „íŠ¸ í˜‘ë ¥ ìš”ì²­ ë„êµ¬ ì¶”ê°€
                tools_for_openai.append({
                    "type": "function",
                    "function": {
                        "name": "request_agent_help",
                        "description": f"""ë‹¤ë¥¸ ì „ë¬¸ ì—ì´ì „íŠ¸ì—ê²Œ í˜‘ë ¥ì„ ìš”ì²­í•©ë‹ˆë‹¤.
ìì‹ ì˜ ì „ë¬¸ ì˜ì—­ì´ ì•„ë‹Œ ì‘ì—…ì´ í•„ìš”í•  ë•Œ ì‚¬ìš©í•˜ì„¸ìš”.

ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸:
{agents_info_str}

ì˜ˆì‹œ: ë²•ë¥  ì—ì´ì „íŠ¸ê°€ ìµœì‹  ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•´ì•¼ í•  ë•Œ search ì—ì´ì „íŠ¸ì—ê²Œ ìš”ì²­""",
                        "parameters": {
                            "type": "object",
                            "properties": {
                                "target_agent": {
                                    "type": "string",
                                    "enum": available_agents,
                                    "description": "í˜‘ë ¥ì„ ìš”ì²­í•  ì—ì´ì „íŠ¸ ì´ë¦„"
                                },
                                "task": {
                                    "type": "string",
                                    "description": "ìš”ì²­í•  ì‘ì—… ë‚´ìš© (êµ¬ì²´ì ìœ¼ë¡œ)"
                                },
                                "reason": {
                                    "type": "string",
                                    "description": "í˜‘ë ¥ì´ í•„ìš”í•œ ì´ìœ "
                                }
                            },
                            "required": ["target_agent", "task"]
                        },
                    },
                })
                print(f"    ğŸ¤ A2A Phase 2: request_agent_help ë„êµ¬ í™œì„±í™”", flush=True)
                print(f"       í˜‘ë ¥ ê°€ëŠ¥ ì—ì´ì „íŠ¸: {', '.join(available_agents)}", flush=True)

        print(f"", flush=True)

        # ì´ì „ ì—ì´ì „íŠ¸ ê²°ê³¼ë¥¼ ëª…í™•í•˜ê²Œ í¬í•¨
        enhanced_input = user_input

        if context:
            print(f"    ğŸ” Context keys: {list(context.keys())}", flush=True)

            if "previous_agent_results" in context:
                previous_results = context["previous_agent_results"]
                print(f"    ğŸ“¦ Previous results count: {len(previous_results)}", flush=True)

                if previous_results:
                    # ì´ì „ ê²°ê³¼ë¥¼ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë³€í™˜
                    previous_info = "\n\n**ğŸ” ì´ì „ ì—ì´ì „íŠ¸ê°€ ì°¾ì€ ì •ë³´:**\n"
                    for idx, result in enumerate(previous_results):
                        agent_name = result.get("agent", "Unknown")
                        response_content = result.get("response", "")
                        previous_info += f"\n[{agent_name}ì˜ ë‹µë³€]\n{response_content}\n"

                        # ì´ì „ ê²°ê³¼ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (ì²« 100ì)
                        preview = response_content[:100].replace('\n', ' ')
                        if len(response_content) > 100:
                            preview += "..."
                        print(f"    ğŸ“„ Previous Result {idx+1}: [{agent_name}] {preview}", flush=True)

                    # ì˜ì¡´ì„± ì§€ì‹œì‚¬í•­ì´ ìˆìœ¼ë©´ ì¶”ê°€
                    if "dependency_instruction" in context:
                        dependency = context["dependency_instruction"]
                        previous_info += f"\n**âš ï¸ ì¤‘ìš”:** {dependency}\n"
                        previous_info += "ìœ„ì˜ ì •ë³´ë¥¼ ë°˜ë“œì‹œ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.\n"
                        print(f"    âš ï¸  Dependency: {dependency}", flush=True)

                    enhanced_input = previous_info + "\n" + "â”€"*70 + f"\n\n**ğŸ“Œ í˜„ì¬ ì§ˆë¬¸:**\n{user_input}"
                    print(f"    âœ… Enhanced input prepared ({len(enhanced_input)} chars)", flush=True)
                else:
                    print(f"    âš ï¸  Previous results is empty", flush=True)
            else:
                print(f"    âš ï¸  No previous_agent_results in context", flush=True)

        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": enhanced_input}
        ]

        max_iterations = int(os.environ.get("MAX_ITERATIONS", 10))
        for iteration in range(max_iterations):
            try:
                response = self.llm_client.chat_completion(messages, tools=tools_for_openai, stream=False)
                choice = response.choices[0].message

                if not getattr(choice, "tool_calls", None):
                    return AgentResponse(
                        content=choice.content or "",
                        metadata={"iterations": iteration + 1},
                        success=True,
                        agent_name=self.name
                    )

                print(f"  ğŸ”§ [{self.name}] Tool calls: {len(choice.tool_calls)}", flush=True)
                tool_results = []

                for tc in choice.tool_calls:
                    tool_name = tc.function.name
                    args = json.loads(tc.function.arguments) if tc.function.arguments else {}

                    # Tool í˜¸ì¶œ input ì¶œë ¥
                    print(f"    ğŸ” [{tool_name}] Input arguments:", flush=True)
                    try:
                        args_str = json.dumps(args, indent=6, ensure_ascii=False)
                        print(f"{args_str}", flush=True)
                    except:
                        print(f"      {args}", flush=True)

                    # ğŸ¤ A2A Phase 2: request_agent_help ì²˜ë¦¬
                    if tool_name == "request_agent_help":
                        target_agent_name = args.get("target_agent", "")
                        task = args.get("task", "")
                        reason = args.get("reason", "")
                        
                        print(f"    ğŸ¤ [{self.name}] â†’ [{target_agent_name}] í˜‘ë ¥ ìš”ì²­", flush=True)
                        print(f"       ì‘ì—…: {task}", flush=True)
                        if reason:
                            print(f"       ì´ìœ : {reason}", flush=True)
                        
                        if self.orchestrator and target_agent_name in self.orchestrator.specialist_agents:
                            target_agent = self.orchestrator.specialist_agents[target_agent_name]
                            try:
                                # ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì—ê²Œ ì‘ì—… ìœ„ì„
                                agent_response = await target_agent.process_with_tools(task, context)
                                
                                if agent_response.success:
                                    tool_results.append({
                                        "tool_call_id": tc.id,
                                        "content": f"[{target_agent_name} ì—ì´ì „íŠ¸ ì‘ë‹µ]\n{agent_response.content}",
                                    })
                                    print(f"    âœ… [{target_agent_name}] í˜‘ë ¥ ì™„ë£Œ", flush=True)
                                else:
                                    tool_results.append({
                                        "tool_call_id": tc.id,
                                        "content": f"Error: {target_agent_name} ì—ì´ì „íŠ¸ í˜‘ë ¥ ì‹¤íŒ¨ - {agent_response.content}",
                                    })
                                    print(f"    âŒ [{target_agent_name}] í˜‘ë ¥ ì‹¤íŒ¨", flush=True)
                            except Exception as e:
                                tool_results.append({
                                    "tool_call_id": tc.id,
                                    "content": f"Error: í˜‘ë ¥ ìš”ì²­ ì‹¤íŒ¨ - {str(e)}",
                                })
                                print(f"    âŒ í˜‘ë ¥ ìš”ì²­ ì‹¤íŒ¨: {str(e)}", flush=True)
                        else:
                            tool_results.append({
                                "tool_call_id": tc.id,
                                "content": f"Error: Agent '{target_agent_name}' not found",
                            })
                            print(f"    âš ï¸  Agent '{target_agent_name}' not found", flush=True)
                        continue

                    tool_found = False
                    for tool in self.tools:
                        current_tool_name = getattr(tool, 'name', type(tool).__name__)
                        if current_tool_name == tool_name:
                            tool_found = True
                            try:
                                from google.adk.models import LlmRequest
                                class DummyToolContext:
                                    def __init__(self):
                                        self.llm_request = LlmRequest(contents=[])

                                tool_context = DummyToolContext()
                                result = await tool.run_async(args=args, tool_context=tool_context)

                                tool_results.append({
                                    "tool_call_id": tc.id,
                                    "content": str(result),
                                })

                                result_str = str(result)
                                result_preview = result_str[:500].replace('\n', ' ')
                                if len(result_str) > 500:
                                    result_preview += "..."
                                print(f"    ğŸ“„ Result preview: {result_preview}", flush=True)
                                print(f"    âœ… Tool {tool_name} executed", flush=True)
                                break
                            except Exception as e:
                                print(f"    âŒ Tool {tool_name} failed: {str(e)}", flush=True)
                                tool_results.append({
                                    "tool_call_id": tc.id,
                                    "content": f"Error: {str(e)}",
                                })
                                break

                    # ë„êµ¬ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°ì—ë„ ê²°ê³¼ ì¶”ê°€
                    if not tool_found:
                        print(f"    âš ï¸  Tool '{tool_name}' not found", flush=True)
                        tool_results.append({
                            "tool_call_id": tc.id,
                            "content": f"Error: Tool '{tool_name}' not found",
                        })

                messages.append({
                    "role": "assistant",
                    "content": choice.content or "",
                    "tool_calls": [tc.model_dump() for tc in choice.tool_calls],
                })
                for tr in tool_results:
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tr["tool_call_id"],
                        "content": tr["content"],
                    })

            except Exception as e:
                return AgentResponse(
                    content=f"ì—ëŸ¬ ë°œìƒ: {str(e)}",
                    metadata={"error": str(e), "iteration": iteration},
                    success=False,
                    agent_name=self.name
                )

        return AgentResponse(
            content="ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ë„ë‹¬",
            metadata={"iterations": max_iterations},
            success=False,
            agent_name=self.name
        )

