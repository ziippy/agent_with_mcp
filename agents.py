# -*- coding: utf-8 -*-
"""
Agent Classes Module
ë‹¤ì–‘í•œ ì—ì´ì „íŠ¸ í´ë˜ìŠ¤ ì •ì˜
"""

import json
from typing import Any, Dict, List, Optional

from llm_client import LLMClient, ContentFilterError
from google.adk.tools import BaseTool


class AgentResponse:
    """ì—ì´ì „íŠ¸ ì‘ë‹µ"""
    def __init__(self, content: str, metadata: Dict[str, Any], success: bool, agent_name: str):
        self.content = content
        self.metadata = metadata
        self.success = success
        self.agent_name = agent_name


class SpecializedAgent:
    """íŠ¹í™”ëœ ì—ì´ì „íŠ¸ ê¸°ë³¸ í´ë˜ìŠ¤"""

    def __init__(self, name: str, role: str, system_prompt: str, llm_client: LLMClient):
        self.name = name
        self.role = role
        self.system_prompt = system_prompt
        self.llm_client = llm_client

    async def process(self, user_input: str, context: Optional[Dict[str, Any]] = None) -> AgentResponse:
        """ì—ì´ì „íŠ¸ ì²˜ë¦¬"""
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": user_input}
        ]

        if context:
            context_str = f"\n\n[Context from previous agents]\n{json.dumps(context, indent=2, ensure_ascii=False)}"
            messages[-1]["content"] += context_str

        try:
            response = self.llm_client.chat_completion(messages, stream=False)
            content = response.choices[0].message.content or ""

            return AgentResponse(
                content=content,
                metadata={"tokens": response.usage.total_tokens if hasattr(response, 'usage') else 0},
                success=True,
                agent_name=self.name
            )
        except ContentFilterError as e:
            return AgentResponse(
                content=f"ì½˜í…ì¸  í•„í„°ë§ ì°¨ë‹¨: {', '.join(e.filtered_categories)}",
                metadata={"error": str(e)},
                success=False,
                agent_name=self.name
            )
        except Exception as e:
            return AgentResponse(
                content=f"ì—ëŸ¬ ë°œìƒ: {str(e)}",
                metadata={"error": str(e)},
                success=False,
                agent_name=self.name
            )


class QuestionUnderstandingAgent(SpecializedAgent):
    """Agent A: ì§ˆë¬¸ ì´í•´ ë° ë¼ìš°íŒ… ë‹´ë‹¹"""

    def __init__(self, llm_client: LLMClient, available_agents: List[str], agent_tools_info: Dict[str, List[str]]):
        agents_str = ", ".join(available_agents)

        # ê° ì—ì´ì „íŠ¸ì˜ ë„êµ¬ ì •ë³´ë¥¼ ë¬¸ìì—´ë¡œ í¬ë§·
        tools_info_str = ""
        for agent, tools in agent_tools_info.items():
            tools_list = ", ".join(tools[:5])
            if len(tools) > 5:
                tools_list += f"... (ì´ {len(tools)}ê°œ)"
            tools_info_str += f"\n  - {agent}: {tools_list}"

        system_prompt = f"""ë‹¹ì‹ ì€ ì§ˆë¬¸ ë¶„ì„ ë° ë¼ìš°íŒ… ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ì „ë¬¸ ì—ì´ì „íŠ¸ì—ê²Œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤.

**ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸ ëª©ë¡:**
{agents_str}

**ê° ì—ì´ì „íŠ¸ê°€ ì œê³µí•˜ëŠ” ë„êµ¬:**{tools_info_str}

**âš ï¸ ë§¤ìš° ì¤‘ìš” - execution_order ì‘ì„± ê·œì¹™:**
1. execution_orderì—ëŠ” **ë°˜ë“œì‹œ ì—ì´ì „íŠ¸ ì´ë¦„ë§Œ** ì‚¬ìš©í•˜ì„¸ìš”
2. ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸ ì´ë¦„: {agents_str}
3. ë„êµ¬ ì´ë¦„ì„ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”

**ë³‘ë ¬ vs ìˆœì°¨ ì‹¤í–‰ íŒë‹¨:**
- ë³‘ë ¬: ë‘ ì‘ì—…ì´ ë…ë¦½ì  â†’ execution_order: [["agent1", "agent2"]]
- ìˆœì°¨: ë‘ ë²ˆì§¸ê°€ ì²« ë²ˆì§¸ ê²°ê³¼ í•„ìš” â†’ execution_order: [["agent1"], ["agent2"]]

**ì‘ë‹µ í˜•ì‹ (JSON):**
{{
  "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2"],
  "question_type": "single|multiple|parallel|general",
  "execution_order": [["agent_name1"], ["agent_name2"]],
  "queries": {{
    "agent_name1": "êµ¬ì²´ì ì¸ ì§ˆë¬¸",
    "agent_name2": "êµ¬ì²´ì ì¸ ì§ˆë¬¸"
  }},
  "dependencies": {{
    "agent_name": "ì˜ì¡´ì„± ì„¤ëª… (ì„ íƒì‚¬í•­)"
  }},
  "analysis": "ì§ˆë¬¸ ë¶„ì„ ë° ì‹¤í–‰ ìˆœì„œ ê²°ì • ì´ìœ "
}}"""

        super().__init__(
            name="QuestionUnderstandingAgent",
            role="ì§ˆë¬¸ ì´í•´ ë° ë¼ìš°íŒ…",
            system_prompt=system_prompt,
            llm_client=llm_client
        )


class ToolBasedAgent(SpecializedAgent):
    """ë„êµ¬ ê¸°ë°˜ ì „ë¬¸ ì—ì´ì „íŠ¸"""

    def __init__(self, name: str, role: str, llm_client: LLMClient, tools: List[BaseTool]):
        # ë„êµ¬ ì •ë³´ ìˆ˜ì§‘
        tools_info = []
        for tool in tools:
            tool_name = getattr(tool, 'name', type(tool).__name__)
            tool_input_schema = getattr(tool, 'input_schema', None)
            if tool_input_schema and 'properties' in tool_input_schema:
                params = list(tool_input_schema['properties'].keys())
                required = tool_input_schema.get('required', [])
                params_str = ', '.join([f"{p}{'*' if p in required else ''}" for p in params])
                tools_info.append(f"  - {tool_name}({params_str})")

        tools_detail = "\n".join(tools_info) if tools_info else "(ë„êµ¬ ì •ë³´ ì—†ìŒ)"

        system_prompt = f"""ë‹¹ì‹ ì€ {role} ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ì¤‘ìš”: ë„êµ¬ ì‚¬ìš© ì‹œ ë°˜ë“œì‹œ ì•„ë˜ì˜ ì •í™•í•œ íŒŒë¼ë¯¸í„° ì´ë¦„ì„ ì‚¬ìš©í•˜ì„¸ìš”!**

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬: {len(tools)}ê°œ
{tools_detail}
(*í‘œì‹œëŠ” í•„ìˆ˜ íŒŒë¼ë¯¸í„°)

**ë„êµ¬ íŒŒë¼ë¯¸í„° ê·œì¹™:**
1. ìŠ¤í‚¤ë§ˆì— ì •ì˜ëœ **ì •í™•í•œ íŒŒë¼ë¯¸í„° ì´ë¦„** ì‚¬ìš©
2. 'keyword' ëŒ€ì‹  'query', 'search_text' ë“± ì‹¤ì œ ì •ì˜ëœ ì´ë¦„ ì‚¬ìš©
3. í•„ìˆ˜ íŒŒë¼ë¯¸í„°(*)ëŠ” ë°˜ë“œì‹œ í¬í•¨

í•„ìš”ì‹œ ì œê³µëœ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ë³´ë¥¼ ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."""

        super().__init__(
            name=name,
            role=role,
            system_prompt=system_prompt,
            llm_client=llm_client
        )
        self.tools = tools

    async def process_with_tools(self, user_input: str, context: Optional[Dict[str, Any]] = None):
        """ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì²˜ë¦¬"""
        tools_for_openai = []

        print(f"\n  ğŸ“‹ [{self.name}] ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ Schema:", flush=True)

        for tool in self.tools:
            tool_name = getattr(tool, 'name', type(tool).__name__)
            tool_description = getattr(tool, 'description', '')
            tool_input_schema = getattr(tool, 'input_schema', None) or {"type": "object", "properties": {}}

            '''
            # Schema ìƒì„¸ ì¶œë ¥
            print(f"    â€¢ {tool_name}:", flush=True)
            print(f"      ì„¤ëª…: {tool_description}", flush=True)
            if tool_input_schema and 'properties' in tool_input_schema:
                print(f"      íŒŒë¼ë¯¸í„°:", flush=True)
                for param_name, param_info in tool_input_schema['properties'].items():
                    param_type = param_info.get('type', 'unknown')
                    param_desc = param_info.get('description', '')
                    param_enum = param_info.get('enum', None)
                    required = param_name in tool_input_schema.get('required', [])
                    req_str = " (í•„ìˆ˜)" if required else " (ì„ íƒ)"
                    enum_str = f" enum={param_enum}" if param_enum else ""
                    print(f"        - {param_name}: {param_type}{enum_str}{req_str} - {param_desc}", flush=True)
            '''

            tools_for_openai.append({
                "type": "function",
                "function": {
                    "name": tool_name,
                    "description": tool_description or "",
                    "parameters": tool_input_schema,
                },
            })

        print(f"", flush=True)

        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": user_input}
        ]

        if context:
            context_str = f"\n\n[Context]\n{json.dumps(context, indent=2, ensure_ascii=False)}"
            messages[-1]["content"] += context_str

        max_iterations = 10
        for iteration in range(max_iterations):
            try:
                response = self.llm_client.chat_completion(messages, tools=tools_for_openai, stream=False)
                choice = response.choices[0].message

                if not getattr(choice, "tool_calls", None):
                    return AgentResponse(
                        content=choice.content or "",
                        metadata={"iterations": iteration + 1},
                        success=True,
                        agent_name=self.name
                    )

                print(f"  ğŸ”§ [{self.name}] Tool calls: {len(choice.tool_calls)}", flush=True)
                tool_results = []

                for tc in choice.tool_calls:
                    tool_name = tc.function.name
                    args = json.loads(tc.function.arguments) if tc.function.arguments else {}

                    # Tool í˜¸ì¶œ input ì¶œë ¥
                    print(f"    ğŸ” [{tool_name}] Input arguments:", flush=True)
                    try:
                        args_str = json.dumps(args, indent=6, ensure_ascii=False)
                        print(f"{args_str}", flush=True)
                    except:
                        print(f"      {args}", flush=True)

                    for tool in self.tools:
                        current_tool_name = getattr(tool, 'name', type(tool).__name__)
                        if current_tool_name == tool_name:
                            try:
                                from google.adk.models import LlmRequest
                                class DummyToolContext:
                                    def __init__(self):
                                        self.llm_request = LlmRequest(contents=[])

                                tool_context = DummyToolContext()
                                result = await tool.run_async(args=args, tool_context=tool_context)
                                tool_results.append({
                                    "tool_call_id": tc.id,
                                    "content": str(result),
                                })
                                print(f"    âœ… Tool {tool_name} executed", flush=True)
                                break
                            except Exception as e:
                                tool_results.append({
                                    "tool_call_id": tc.id,
                                    "content": f"Error: {str(e)}",
                                })
                                break

                messages.append({
                    "role": "assistant",
                    "content": choice.content or "",
                    "tool_calls": [tc.model_dump() for tc in choice.tool_calls],
                })
                for tr in tool_results:
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tr["tool_call_id"],
                        "content": tr["content"],
                    })

            except Exception as e:
                return AgentResponse(
                    content=f"ì—ëŸ¬ ë°œìƒ: {str(e)}",
                    metadata={"error": str(e), "iteration": iteration},
                    success=False,
                    agent_name=self.name
                )

        return AgentResponse(
            content="ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ë„ë‹¬",
            metadata={"iterations": max_iterations},
            success=False,
            agent_name=self.name
        )

