# -*- coding: utf-8 -*-
import os
import json
import asyncio
import time
from typing import Any, Dict, List, Optional
from dataclasses import dataclass

from dotenv import load_dotenv
from openai import AzureOpenAI, OpenAI, BadRequestError

try:
    import anthropic
except ImportError:
    anthropic = None

try:
    import google.generativeai as genai
except ImportError:
    genai = None

from google.adk.tools import BaseTool
from google.adk.tools.mcp_tool.mcp_toolset import McpToolset
from google.adk.tools.mcp_tool.mcp_session_manager import StreamableHTTPConnectionParams

load_dotenv()


class ContentFilterError(Exception):
    """LLM ì½˜í…ì¸  í•„í„°ë§ ì—ëŸ¬"""
    def __init__(self, filtered_categories: List[str], original_error: Exception):
        self.filtered_categories = filtered_categories
        self.original_error = original_error
        super().__init__(f"Content filtered: {', '.join(filtered_categories)}")


class LLMClient:
    """ë²”ìš© LLM í´ë¼ì´ì–¸íŠ¸ - Azure OpenAI, OpenAI, vLLM, Google Gemini, Anthropic Claude, xAI Grok ì§€ì›"""

    def __init__(
        self,
        provider: str = "azure",  # "azure", "openai", "vllm", "google", "anthropic", "xai"
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        model: Optional[str] = None,
        # Azure OpenAI ì „ìš©
        api_version: Optional[str] = None,
        azure_endpoint: Optional[str] = None,
        deployment: Optional[str] = None,
    ):
        """
        LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”

        Args:
            provider: LLM ì œê³µì ("azure", "openai", "vllm", "google", "anthropic", "xai")
            api_key: API í‚¤
            base_url: ê¸°ë³¸ URL (OpenAI, vLLM, Google, Anthropic, xAIìš©)
            model: ëª¨ë¸ ì´ë¦„ (OpenAI, vLLM, Google, Anthropic, xAIìš©)
            api_version: API ë²„ì „ (Azure OpenAIìš©)
            azure_endpoint: Azure ì—”ë“œí¬ì¸íŠ¸ (Azure OpenAIìš©)
            deployment: ë°°í¬ ì´ë¦„ (Azure OpenAIìš©)
        """
        self.provider = provider.lower()
        self.model = model or deployment

        try:
            if self.provider == "azure":
                self.client = AzureOpenAI(
                    api_key=api_key,
                    api_version=api_version,
                    azure_endpoint=azure_endpoint,
                )
                self.model = deployment
                print(f"[LLM] Azure OpenAI initialized (deployment: {deployment})")

            elif self.provider == "openai":
                # OpenAI ì´ˆê¸°í™” - base_urlì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
                init_kwargs = {
                    "api_key": api_key,
                }
                if base_url:  # base_urlì´ ìˆì„ ë•Œë§Œ ì „ë‹¬
                    init_kwargs["base_url"] = base_url

                self.client = OpenAI(**init_kwargs)

                # ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
                print(f"[LLM] OpenAI initialized")
                print(f"[LLM]   Model: {model}")
                print(f"[LLM]   API Key: {api_key[:20]}..." if api_key else "[LLM]   API Key: None")
                print(f"[LLM]   Base URL: {base_url if base_url else 'default (https://api.openai.com/v1)'}")

            elif self.provider == "vllm":
                # vLLMì€ OpenAI í˜¸í™˜ APIë¥¼ ì œê³µ
                # UTF-8 ì¸ì½”ë”© ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ëª…ì‹œì  ì„¤ì •
                import httpx
                import json as json_lib

                # UTF-8ì„ ê°•ì œí•˜ëŠ” ì»¤ìŠ¤í…€ JSON serializer
                def utf8_json_serializer(obj):
                    return json_lib.dumps(obj, ensure_ascii=False).encode('utf-8')

                # UTF-8ì„ ì§€ì›í•˜ëŠ” HTTP í´ë¼ì´ì–¸íŠ¸ ìƒì„±
                http_client = httpx.Client(
                    timeout=httpx.Timeout(60.0, connect=10.0),
                    limits=httpx.Limits(max_keepalive_connections=5, max_connections=10),
                    headers={
                        "Content-Type": "application/json; charset=utf-8",
                        "Accept": "application/json",
                        "Accept-Charset": "utf-8"
                    }
                )

                # httpxì˜ ê¸°ë³¸ JSON serializerë¥¼ UTF-8ì„ ì§€ì›í•˜ëŠ” ê²ƒìœ¼ë¡œ êµì²´
                # í•˜ì§€ë§Œ OpenAI í´ë¼ì´ì–¸íŠ¸ëŠ” ì´ë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ
                # ë©”ì‹œì§€ ì „ì²˜ë¦¬ë¥¼ chat_completionì—ì„œ ìˆ˜í–‰

                self.client = OpenAI(
                    api_key=api_key or "EMPTY",  # vLLMì€ API í‚¤ê°€ í•„ìš”ì—†ì„ ìˆ˜ ìˆìŒ
                    base_url=base_url,
                    http_client=http_client,
                    default_headers={
                        "Content-Type": "application/json; charset=utf-8"
                    }
                )

                # OpenAI í´ë¼ì´ì–¸íŠ¸ ë‚´ë¶€ì˜ JSON serializationì„ íŒ¨ì¹˜
                # ì´ê²ƒì´ ê°€ì¥ í™•ì‹¤í•œ UTF-8 ì¸ì½”ë”© ë³´ì¥ ë°©ë²•
                import functools
                original_dumps = json_lib.dumps

                @functools.wraps(original_dumps)
                def utf8_dumps(*args, **kwargs):
                    # ensure_ascii=Falseë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
                    kwargs.setdefault('ensure_ascii', False)
                    return original_dumps(*args, **kwargs)

                # json.dumpsë¥¼ íŒ¨ì¹˜ (vLLM ì‚¬ìš© ì‹œì—ë§Œ)
                json_lib.dumps = utf8_dumps

                print(f"[LLM] vLLM initialized (base_url: {base_url}, model: {model})")
                print(f"[LLM] UTF-8 encoding explicitly configured for vLLM")
                print(f"[LLM] JSON serialization patched for UTF-8 support")

            elif self.provider == "google":
                if genai is None:
                    raise ImportError("google-generativeai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install google-generativeai'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")

                # base_urlì´ ìˆìœ¼ë©´ OpenAI í˜¸í™˜ ëª¨ë“œ ì‚¬ìš©
                if base_url:
                    # Gemini OpenAI compatibility mode
                    self.client = OpenAI(
                        api_key=api_key,
                        base_url=base_url,
                    )
                    self.model = model or "gemini-2.0-flash-exp"
                    self.use_openai_compat = True
                    print(f"[LLM] Google Gemini initialized (OpenAI compatibility mode)")
                    print(f"[LLM]   Model: {self.model}")
                    print(f"[LLM]   Base URL: {base_url}")
                else:
                    # ë„¤ì´í‹°ë¸Œ Gemini SDK ì‚¬ìš©
                    genai.configure(api_key=api_key)
                    self.client = genai.GenerativeModel(model or "gemini-1.5-pro")
                    self.model = model or "gemini-1.5-pro"
                    self.use_openai_compat = False
                    print(f"[LLM] Google Gemini initialized (Native SDK mode, model: {self.model})")

            elif self.provider == "anthropic":
                if anthropic is None:
                    raise ImportError("anthropic íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install anthropic'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")

                # base_urlì´ ìˆìœ¼ë©´ OpenAI í˜¸í™˜ ëª¨ë“œ ì‚¬ìš© ê°€ëŠ¥
                if base_url:
                    # Anthropicë„ OpenAI í˜¸í™˜ APIë¥¼ ì œê³µí•  ìˆ˜ ìˆìŒ
                    self.client = OpenAI(
                        api_key=api_key,
                        base_url=base_url,
                    )
                    self.model = model or "claude-3-5-sonnet-20241022"
                    self.use_openai_compat = True
                    print(f"[LLM] Anthropic Claude initialized (OpenAI compatibility mode)")
                    print(f"[LLM]   Model: {self.model}")
                    print(f"[LLM]   Base URL: {base_url}")
                else:
                    # ë„¤ì´í‹°ë¸Œ Anthropic SDK ì‚¬ìš©
                    self.client = anthropic.Anthropic(api_key=api_key)
                    self.model = model or "claude-3-5-sonnet-20241022"
                    self.use_openai_compat = False
                    print(f"[LLM] Anthropic Claude initialized (Native SDK mode, model: {self.model})")

            elif self.provider == "xai":
                # xAI Grokì€ OpenAI í˜¸í™˜ API ì œê³µ
                self.client = OpenAI(
                    api_key=api_key,
                    base_url=base_url or "https://api.x.ai/v1",
                )
                self.model = model or "grok-beta"
                print(f"[LLM] xAI Grok initialized")
                print(f"[LLM]   Model: {self.model}")
                print(f"[LLM]   Base URL: {base_url or 'https://api.x.ai/v1'}")

            else:
                raise ValueError(f"Unsupported provider: {provider}. Use 'azure', 'openai', 'vllm', 'google', 'anthropic', or 'xai'")

        except Exception as e:
            print(f"[LLM] Initialization failed -> {e}")
            raise

    def chat_completion(self, messages: List[Dict[str, Any]], tools: Optional[List[Dict[str, Any]]] = None, stream: bool = False):
        """LLM Chat Completion í˜¸ì¶œ"""
        try:
            # Google Gemini (Native SDK)
            if self.provider == "google" and not getattr(self, 'use_openai_compat', False):
                return self._gemini_chat_completion(messages, tools, stream)

            # Anthropic Claude (Native SDK)
            elif self.provider == "anthropic" and not getattr(self, 'use_openai_compat', False):
                return self._claude_chat_completion(messages, tools, stream)

            # vLLMì˜ ê²½ìš° í•œê¸€ ë“± non-ASCII ë¬¸ì ì²˜ë¦¬ë¥¼ ìœ„í•œ ì¶”ê°€ ì²˜ë¦¬
            elif self.provider == "vllm":
                # ë©”ì‹œì§€ë¥¼ JSONìœ¼ë¡œ ì§ë ¬í™”í•œ í›„ ë‹¤ì‹œ íŒŒì‹±í•˜ì—¬ UTF-8 ì¸ì½”ë”© ë³´ì¥
                import json as json_lib
                try:
                    # ensure_ascii=Falseë¡œ UTF-8 ë¬¸ì ë³´ì¡´
                    messages_json = json_lib.dumps(messages, ensure_ascii=False)
                    messages = json_lib.loads(messages_json)
                except Exception as e:
                    print(f"[LLM] Warning: Failed to process messages for UTF-8: {e}")

            # OpenAI í˜¸í™˜ API (Azure, OpenAI, vLLM, Google OpenAI-compat, Anthropic OpenAI-compat, xAI)
            kwargs = {
                "model": self.model,
                "messages": messages,
                "temperature": 0.2,
                "stream": stream,
            }

            if tools:
                kwargs["tools"] = tools
                kwargs["tool_choice"] = "auto"

            return self.client.chat.completions.create(**kwargs)

        except UnicodeEncodeError as e:
            # ì¸ì½”ë”© ì—ëŸ¬ ì²˜ë¦¬
            error_msg = f"ì¸ì½”ë”© ì—ëŸ¬: {str(e)}\n"
            error_msg += f"Provider: {self.provider}\n"
            error_msg += f"Model: {self.model}\n"
            error_msg += "vLLM ì‚¬ìš© ì‹œ ëª¨ë¸ì´ UTF-8ì„ ì§€ì›í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."
            raise Exception(error_msg)

        except BadRequestError as e:
            error_str = str(e)

            # OpenAI API ì—ëŸ¬ ì²˜ë¦¬
            if hasattr(e, 'status_code'):
                if e.status_code == 429:
                    raise Exception(f"OpenAI API í• ë‹¹ëŸ‰ ì´ˆê³¼ ë˜ëŠ” Rate Limit: {error_str}\n"
                                  f"Provider: {self.provider}\n"
                                  f"Model: {self.model}\n"
                                  f"í•´ê²° ë°©ë²•:\n"
                                  f"1. API í‚¤ì˜ í¬ë ˆë”§ì„ í™•ì¸í•˜ì„¸ìš”\n"
                                  f"2. ì˜¬ë°”ë¥¸ API í‚¤ê°€ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”\n"
                                  f"3. Rate Limitì¸ ê²½ìš° ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”")
                elif e.status_code == 401:
                    raise Exception(f"OpenAI API ì¸ì¦ ì‹¤íŒ¨: {error_str}\n"
                                  f"API í‚¤ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ì„¸ìš”")
                elif e.status_code == 404:
                    raise Exception(f"ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {error_str}\n"
                                  f"Model: {self.model}\n"
                                  f"ì˜¬ë°”ë¥¸ ëª¨ë¸ëª…ì„ ì‚¬ìš©í•˜ê³  ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”\n"
                                  f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo")

            # Azure OpenAI ì½˜í…ì¸  í•„í„°ë§ ì²˜ë¦¬
            if 'content_filter' in error_str or 'content management policy' in error_str.lower():
                error_body = getattr(e, 'body', None)
                filtered_categories = []

                if error_body and isinstance(error_body, dict):
                    error_info = error_body.get('error', {})
                    inner_error = error_info.get('innererror', {})
                    filter_result = inner_error.get('content_filter_result', {})

                    for category, details in filter_result.items():
                        if isinstance(details, dict) and details.get('filtered'):
                            severity = details.get('severity', 'unknown')
                            filtered_categories.append(f"{category}={severity}")

                if not filtered_categories:
                    filtered_categories = ['content_filter']

                raise ContentFilterError(
                    filtered_categories=filtered_categories,
                    original_error=e
                )

            raise

        except Exception as e:
            # ê¸°íƒ€ ì—ëŸ¬ì— ëŒ€í•œ ë””ë²„ê¹… ì •ë³´ ì¶”ê°€
            if self.provider == "vllm":
                error_msg = f"vLLM í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}\n"
                error_msg += f"Base URL: {self.client.base_url}\n"
                error_msg += f"Model: {self.model}\n"
                error_msg += "vLLM ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€, ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."
                raise Exception(error_msg) from e
            raise

    def _gemini_chat_completion(self, messages: List[Dict[str, Any]], tools: Optional[List[Dict[str, Any]]] = None, stream: bool = False):
        """Gemini API í˜¸ì¶œ"""
        # GeminiëŠ” OpenAIì™€ ë‹¤ë¥¸ ë©”ì‹œì§€ í˜•ì‹ ì‚¬ìš©
        # system ë©”ì‹œì§€ë¥¼ ë¶„ë¦¬í•˜ê³  user/assistant ë©”ì‹œì§€ë§Œ ì „ë‹¬
        system_instruction = None
        chat_messages = []

        for msg in messages:
            if msg["role"] == "system":
                system_instruction = msg["content"]
            elif msg["role"] == "user":
                chat_messages.append({"role": "user", "parts": [msg["content"]]})
            elif msg["role"] == "assistant":
                chat_messages.append({"role": "model", "parts": [msg["content"]]})

        # ìƒˆ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (system instruction í¬í•¨)
        if system_instruction:
            model = genai.GenerativeModel(
                model_name=self.model,
                system_instruction=system_instruction
            )
        else:
            model = self.client

        # Tool callingì€ Geminiì—ì„œ ë³„ë„ ì²˜ë¦¬ í•„ìš” (ì—¬ê¸°ì„œëŠ” ê¸°ë³¸ êµ¬í˜„)
        response = model.generate_content(
            chat_messages[-1]["parts"] if chat_messages else "",
            generation_config=genai.types.GenerationConfig(temperature=0.2)
        )

        # OpenAI í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        class GeminiResponse:
            def __init__(self, text):
                self.choices = [type('obj', (object,), {
                    'message': type('obj', (object,), {
                        'content': text,
                        'role': 'assistant'
                    })()
                })()]

        return GeminiResponse(response.text)

    def _claude_chat_completion(self, messages: List[Dict[str, Any]], tools: Optional[List[Dict[str, Any]]] = None, stream: bool = False):
        """Claude API í˜¸ì¶œ"""
        # ClaudeëŠ” system ë©”ì‹œì§€ë¥¼ ë³„ë„ íŒŒë¼ë¯¸í„°ë¡œ ë°›ìŒ
        system_message = None
        claude_messages = []

        for msg in messages:
            if msg["role"] == "system":
                system_message = msg["content"]
            else:
                claude_messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })

        kwargs = {
            "model": self.model,
            "messages": claude_messages,
            "max_tokens": 4096,
            "temperature": 0.2,
        }

        if system_message:
            kwargs["system"] = system_message

        # Tool callingì€ Claudeì—ì„œ ë³„ë„ ì²˜ë¦¬ í•„ìš” (ì—¬ê¸°ì„œëŠ” ê¸°ë³¸ êµ¬í˜„)
        if tools:
            # Claudeì˜ tool í˜•ì‹ìœ¼ë¡œ ë³€í™˜ í•„ìš”
            pass

        response = self.client.messages.create(**kwargs)

        # OpenAI í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        class ClaudeResponse:
            def __init__(self, content):
                self.choices = [type('obj', (object,), {
                    'message': type('obj', (object,), {
                        'content': content,
                        'role': 'assistant'
                    })()
                })()]

        return ClaudeResponse(response.content[0].text)


@dataclass
class MCPServerConnection:
    """ë‹¨ì¼ MCP ì„œë²„ ì—°ê²°"""
    name: str
    toolset: McpToolset


@dataclass
class AgentResponse:
    """ì—ì´ì „íŠ¸ ì‘ë‹µ"""
    content: str
    metadata: Dict[str, Any]
    success: bool
    agent_name: str


class SpecializedAgent:
    """íŠ¹í™”ëœ ì—ì´ì „íŠ¸ ê¸°ë³¸ í´ë˜ìŠ¤"""

    def __init__(self, name: str, role: str, system_prompt: str, llm_client: LLMClient):
        self.name = name
        self.role = role
        self.system_prompt = system_prompt
        self.llm_client = llm_client

    async def process(self, user_input: str, context: Optional[Dict[str, Any]] = None) -> AgentResponse:
        """ì—ì´ì „íŠ¸ ì²˜ë¦¬"""
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": user_input}
        ]

        if context:
            context_str = f"\n\n[Context from previous agents]\n{json.dumps(context, indent=2, ensure_ascii=False)}"
            messages[-1]["content"] += context_str

        try:
            response = self.llm_client.chat_completion(messages, stream=False)
            content = response.choices[0].message.content or ""

            return AgentResponse(
                content=content,
                metadata={"tokens": response.usage.total_tokens if hasattr(response, 'usage') else 0},
                success=True,
                agent_name=self.name
            )
        except ContentFilterError as e:
            return AgentResponse(
                content=f"ì½˜í…ì¸  í•„í„°ë§ ì°¨ë‹¨: {', '.join(e.filtered_categories)}",
                metadata={"error": str(e)},
                success=False,
                agent_name=self.name
            )
        except Exception as e:
            return AgentResponse(
                content=f"ì—ëŸ¬ ë°œìƒ: {str(e)}",
                metadata={"error": str(e)},
                success=False,
                agent_name=self.name
            )


class QuestionUnderstandingAgent(SpecializedAgent):
    """Agent A: ì§ˆë¬¸ ì´í•´ ë° ë¼ìš°íŒ… ë‹´ë‹¹"""

    def __init__(self, llm_client: LLMClient, available_agents: List[str], agent_tools_info: Dict[str, List[str]]):
        """
        Args:
            llm_client: LLM í´ë¼ì´ì–¸íŠ¸
            available_agents: ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸ ëª©ë¡ (ì˜ˆ: ["mcp1", "mcp2", "mcp3"])
            agent_tools_info: ê° ì—ì´ì „íŠ¸ì˜ ë„êµ¬ ì •ë³´ {"mcp1": ["tool1", "tool2"], ...}
        """
        agents_str = ", ".join(available_agents)

        # ê° ì—ì´ì „íŠ¸ì˜ ë„êµ¬ ì •ë³´ë¥¼ ë¬¸ìì—´ë¡œ í¬ë§·
        tools_info_str = ""
        for agent, tools in agent_tools_info.items():
            tools_list = ", ".join(tools[:5])  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
            if len(tools) > 5:
                tools_list += f"... (ì´ {len(tools)}ê°œ)"
            tools_info_str += f"\n  - {agent}: {tools_list}"

        system_prompt = f"""ë‹¹ì‹ ì€ ì§ˆë¬¸ ë¶„ì„ ë° ë¼ìš°íŒ… ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ì „ë¬¸ ì—ì´ì „íŠ¸ì—ê²Œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤.

**ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸ ëª©ë¡:**
{agents_str}

**ê° ì—ì´ì „íŠ¸ê°€ ì œê³µí•˜ëŠ” ë„êµ¬:**{tools_info_str}

**âš ï¸ ë§¤ìš° ì¤‘ìš” - execution_order ì‘ì„± ê·œì¹™:**
1. execution_orderì—ëŠ” **ë°˜ë“œì‹œ ì—ì´ì „íŠ¸ ì´ë¦„ë§Œ** ì‚¬ìš©í•˜ì„¸ìš”
2. ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸ ì´ë¦„: {agents_str}
3. ë„êµ¬ ì´ë¦„ì„ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”

**ì—ì´ì „íŠ¸ ì—­í•  ì¶”ë¡  (ë„êµ¬ë¥¼ ë³´ê³  íŒë‹¨):**
ê° ì—ì´ì „íŠ¸ê°€ ì œê³µí•˜ëŠ” ë„êµ¬ë¥¼ ë³´ê³  ì–´ë–¤ ì—­í• ì„ í•˜ëŠ”ì§€ ì¶”ë¡ í•œ í›„, **ì—ì´ì „íŠ¸ ì´ë¦„**ì„ ì‚¬ìš©í•˜ì„¸ìš”.

**ì˜ëª»ëœ ì˜ˆì‹œ (ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€):**
âŒ execution_order: [["law-search", "precedent-search"]]  <- ë„êµ¬ ì´ë¦„ ì‚¬ìš©
âŒ execution_order: [["web-search"]]  <- ë„êµ¬ ì´ë¦„ ì‚¬ìš©

**ì˜¬ë°”ë¥¸ ì˜ˆì‹œ:**
âœ… execution_order: [["{agents_str.split(', ')[0] if agents_str else 'mcp1'}", "{agents_str.split(', ')[1] if ', ' in agents_str else 'mcp2'}"]]  <- ì—ì´ì „íŠ¸ ì´ë¦„ ì‚¬ìš©
âœ… execution_order: [["{agents_str.split(', ')[0] if agents_str else 'mcp1'}"]]  <- ì—ì´ì „íŠ¸ ì´ë¦„ ì‚¬ìš©

**ì¤‘ìš” ì›ì¹™:**
1. ì§ˆë¬¸ì´ ì—¬ëŸ¬ ì—ì´ì „íŠ¸ë¥¼ í•„ìš”ë¡œ í•˜ë©´ **ì‹¤í–‰ ìˆœì„œ**ë¥¼ ë…¼ë¦¬ì ìœ¼ë¡œ ê²°ì •
2. **ë³‘ë ¬ ì‹¤í–‰ì´ ìœ ë¦¬í•œ ê²½ìš° ì—¬ëŸ¬ ì—ì´ì „íŠ¸ë¥¼ ë™ì‹œì— ì‹¤í–‰**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
3. **ë‚˜ì¤‘ ì—ì´ì „íŠ¸ê°€ ì´ì „ ì—ì´ì „íŠ¸ ê²°ê³¼ë¥¼ í™œìš©í•´ì•¼ í•˜ë©´ ì˜ì¡´ì„±ì„ ëª…ì‹œí•˜ì—¬ ìˆœì°¨ ì‹¤í–‰**
4. ì¼ë°˜ì ì¸ ëŒ€í™”ëŠ” ì—ì´ì „íŠ¸ë¥¼ í˜¸ì¶œí•˜ì§€ ì•ŠìŒ (execution_order: [])

**ë³‘ë ¬ vs ìˆœì°¨ ì‹¤í–‰ íŒë‹¨ ê¸°ì¤€:**

ğŸ”€ **ë³‘ë ¬ ì‹¤í–‰ (ë™ì‹œì— ë…ë¦½ì ìœ¼ë¡œ ê²€ìƒ‰):**
- ë‘ ì—ì´ì „íŠ¸ì˜ ì‘ì—…ì´ **ì„œë¡œ ë…ë¦½ì **ì¼ ë•Œ
- í•œ ì—ì´ì „íŠ¸ì˜ ê²°ê³¼ê°€ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì˜ ì…ë ¥ìœ¼ë¡œ í•„ìš”í•˜ì§€ ì•Šì„ ë•Œ
- ì˜ˆ: "12ëŒ€ ì¤‘ê³¼ì‹¤ì€ ë­ì•¼?" â†’ ë²•ë¥  ì¡°ë¬¸ê³¼ íŒë¡€ë¥¼ ë™ì‹œì— ê²€ìƒ‰ ê°€ëŠ¥
  - execution_order: [["agent1", "agent2"]]
  - ë‘ ê²€ìƒ‰ì´ ì„œë¡œ ë…ë¦½ì ì„

â­ï¸ **ìˆœì°¨ ì‹¤í–‰ (ì²« ë²ˆì§¸ ê²°ê³¼ë¥¼ ë‘ ë²ˆì§¸ê°€ í™œìš©):**
- ë‚˜ì¤‘ ì—ì´ì „íŠ¸ê°€ **ì´ì „ ì—ì´ì „íŠ¸ì˜ ê²°ê³¼ë¥¼ ì°¸ê³ **í•´ì•¼ í•  ë•Œ
- "ì°¾ì•„ë³´ê³  ~í•´ì¤˜", "ê²€ìƒ‰í•œ í›„ ~í•´ì¤˜", "ë°”íƒ•ìœ¼ë¡œ ~í•´ì¤˜" ê°™ì€ í‘œí˜„ì´ ìˆì„ ë•Œ
- ì˜ˆ: "ìµœê·¼ ê·¼ë¡œê¸°ì¤€ë²• ìœ„ë°˜ ì‚¬ë¡€ë¥¼ ì°¾ì•„ë³´ê³ , í•´ë‹¹ ë²• ì¡°í•­ì„ ì•Œë ¤ì¤˜"
  - execution_order: [["agent1"], ["agent2"]]
  - agent2ëŠ” agent1ì˜ ê²°ê³¼(ìœ„ë°˜ ì‚¬ë¡€)ë¥¼ ë³´ê³  ê´€ë ¨ ë²• ì¡°í•­ì„ ê²€ìƒ‰í•´ì•¼ í•¨
  - dependencies: {{"agent2": "agent1ì—ì„œ ì°¾ì€ ìœ„ë°˜ ì‚¬ë¡€ë¥¼ ë¶„ì„í•˜ì—¬ í•´ë‹¹ ë²• ì¡°í•­ ê²€ìƒ‰"}}

**ì‘ë‹µ í˜•ì‹ (JSON):**
{{
  "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2"],
  "question_type": "single|multiple|parallel|general",
  "execution_order": [["agent_name1"], ["agent_name2", "agent_name3"]],
  "queries": {{
    "agent_name1": "í•´ë‹¹ ì—ì´ì „íŠ¸ì—ê²Œ í•  êµ¬ì²´ì ì¸ ì§ˆë¬¸",
    "agent_name2": "í•´ë‹¹ ì—ì´ì „íŠ¸ì—ê²Œ í•  êµ¬ì²´ì ì¸ ì§ˆë¬¸"
  }},
  "dependencies": {{
    "agent_name": "ì´ì „ ì—ì´ì „íŠ¸ ê²°ê³¼ í™œìš© ë°©ë²• (ì„ íƒì‚¬í•­)"
  }},
  "analysis": "ì§ˆë¬¸ ë¶„ì„ ë° ì‹¤í–‰ ìˆœì„œ(ìˆœì°¨/ë³‘ë ¬) ê²°ì • ì´ìœ "
}}

**execution_order ì‘ì„± ê·œì¹™:**
âš ï¸ CRITICAL: execution_orderì—ëŠ” ë°˜ë“œì‹œ ì‹¤ì œ ì—ì´ì „íŠ¸ ì´ë¦„ë§Œ ì‚¬ìš©í•˜ì„¸ìš”!
ì‚¬ìš© ê°€ëŠ¥: {agents_str}
ì‚¬ìš© ê¸ˆì§€: ë„êµ¬ ì´ë¦„ (law-search, precedent-search ë“±)

**í˜•ì‹ ì˜ˆì‹œ:**
- `[["{available_agents[0] if available_agents else 'mcp1'}"]]`: ë‹¨ì¼ ì—ì´ì „íŠ¸ ì‹¤í–‰
- `[["{available_agents[0] if available_agents else 'mcp1'}"], ["{available_agents[1] if len(available_agents) > 1 else 'mcp2'}"]]`: ìˆœì°¨ ì‹¤í–‰
- `[["{available_agents[0] if available_agents else 'mcp1'}", "{available_agents[1] if len(available_agents) > 1 else 'mcp2'}"]]`: ë³‘ë ¬ ì‹¤í–‰
- `[]`: ì¼ë°˜ ëŒ€í™” (ì—ì´ì „íŠ¸ ë¯¸í˜¸ì¶œ)

**êµ¬ì²´ì ì¸ ì˜ˆì‹œ ì‹œë‚˜ë¦¬ì˜¤:**

1ï¸âƒ£ **ë³‘ë ¬ ì‹¤í–‰ ì˜ˆì‹œ:**
ì§ˆë¬¸: "12ëŒ€ ì¤‘ê³¼ì‹¤ì´ ë­ì•¼?"
ë¶„ì„: ë²•ë¥  ì¡°ë¬¸ê³¼ íŒë¡€ë¥¼ ë™ì‹œì— ê²€ìƒ‰ ê°€ëŠ¥ (ë…ë¦½ì )
```json
{{
  "question_type": "parallel",
  "execution_order": [["{available_agents[0] if available_agents else 'mcp1'}", "{available_agents[1] if len(available_agents) > 1 else 'mcp2'}"]],
  "queries": {{
    "{available_agents[0] if available_agents else 'mcp1'}": "12ëŒ€ ì¤‘ê³¼ì‹¤ì˜ ë²•ë¥ ì  ì •ì˜ë¥¼ ê²€ìƒ‰",
    "{available_agents[1] if len(available_agents) > 1 else 'mcp2'}": "12ëŒ€ ì¤‘ê³¼ì‹¤ ê´€ë ¨ íŒë¡€ë¥¼ ê²€ìƒ‰"
  }},
  "dependencies": {{}},
  "analysis": "ë²•ë¥  ì¡°ë¬¸ê³¼ íŒë¡€ëŠ” ë…ë¦½ì ìœ¼ë¡œ ê²€ìƒ‰ ê°€ëŠ¥í•˜ë¯€ë¡œ ë³‘ë ¬ ì‹¤í–‰"
}}
```

2ï¸âƒ£ **ìˆœì°¨ ì‹¤í–‰ ì˜ˆì‹œ (ì˜ì¡´ì„± ìˆìŒ):**
ì§ˆë¬¸: "ìµœê·¼ ê·¼ë¡œê¸°ì¤€ë²• ìœ„ë°˜ ì‚¬ë¡€ë¥¼ ì°¾ì•„ë³´ê³ , í•´ë‹¹ ë²• ì¡°í•­ì„ ì•Œë ¤ì¤˜"
ë¶„ì„: ì²« ë²ˆì§¸ë¡œ ì‚¬ë¡€ ê²€ìƒ‰ â†’ ê·¸ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë²• ì¡°í•­ ê²€ìƒ‰ (ì˜ì¡´ì )
```json
{{
  "question_type": "multiple",
  "execution_order": [["{available_agents[0] if available_agents else 'mcp1'}"], ["{available_agents[1] if len(available_agents) > 1 else 'mcp2'}"]],
  "queries": {{
    "{available_agents[0] if available_agents else 'mcp1'}": "ìµœê·¼ ê·¼ë¡œê¸°ì¤€ë²• ìœ„ë°˜ ì‚¬ë¡€ë¥¼ ê²€ìƒ‰",
    "{available_agents[1] if len(available_agents) > 1 else 'mcp2'}": "ìœ„ë°˜ ì‚¬ë¡€ì— í•´ë‹¹í•˜ëŠ” ê·¼ë¡œê¸°ì¤€ë²• ì¡°í•­ì„ ê²€ìƒ‰"
  }},
  "dependencies": {{
    "{available_agents[1] if len(available_agents) > 1 else 'mcp2'}": "ì²« ë²ˆì§¸ ì—ì´ì „íŠ¸ê°€ ì°¾ì€ ìœ„ë°˜ ì‚¬ë¡€ë¥¼ ë¶„ì„í•˜ì—¬ í•´ë‹¹ë˜ëŠ” ë²• ì¡°í•­ ê²€ìƒ‰"
  }},
  "analysis": "ì‚¬ë¡€ë¥¼ ë¨¼ì € ì°¾ì€ í›„, ê·¸ ì‚¬ë¡€ì— í•´ë‹¹í•˜ëŠ” ë²• ì¡°í•­ì„ ê²€ìƒ‰í•´ì•¼ í•˜ë¯€ë¡œ ìˆœì°¨ ì‹¤í–‰"
}}
```"""
        super().__init__(
            name="QuestionUnderstandingAgent",
            role="ì§ˆë¬¸ ì´í•´ ë° ë¼ìš°íŒ…",
            system_prompt=system_prompt,
            llm_client=llm_client
        )


class ToolBasedAgent(SpecializedAgent):
    """ë„êµ¬ ê¸°ë°˜ ì „ë¬¸ ì—ì´ì „íŠ¸ (ë²”ìš©)"""

    def __init__(self, name: str, role: str, llm_client: LLMClient, tools: List[BaseTool]):
        system_prompt = f"""ë‹¹ì‹ ì€ {role} ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ì „ë¬¸ì ì¸ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
í•„ìš”ì‹œ ì œê³µëœ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ë³´ë¥¼ ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬: {len(tools)}ê°œ"""
        super().__init__(
            name=name,
            role=role,
            system_prompt=system_prompt,
            llm_client=llm_client
        )
        self.tools = tools

    async def process_with_tools(self, user_input: str, context: Optional[Dict[str, Any]] = None) -> AgentResponse:
        """ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì²˜ë¦¬"""
        tools_for_openai = []
        for tool in self.tools:
            tool_name = getattr(tool, 'name', type(tool).__name__)
            tool_description = getattr(tool, 'description', '')
            tool_input_schema = getattr(tool, 'input_schema', None) or {"type": "object", "properties": {}}
            tools_for_openai.append({
                "type": "function",
                "function": {
                    "name": tool_name,
                    "description": tool_description or "",
                    "parameters": tool_input_schema,
                },
            })

        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": user_input}
        ]

        if context:
            context_str = f"\n\n[Context]\n{json.dumps(context, indent=2, ensure_ascii=False)}"
            messages[-1]["content"] += context_str

        max_iterations = 10
        for iteration in range(max_iterations):
            try:
                response = self.llm_client.chat_completion(messages, tools=tools_for_openai, stream=False)
                choice = response.choices[0].message

                if not getattr(choice, "tool_calls", None):
                    return AgentResponse(
                        content=choice.content or "",
                        metadata={"iterations": iteration + 1},
                        success=True,
                        agent_name=self.name
                    )

                print(f"  ğŸ”§ [{self.name}] Tool calls: {len(choice.tool_calls)}", flush=True)
                tool_results = []

                for tc in choice.tool_calls:
                    tool_name = tc.function.name
                    args = json.loads(tc.function.arguments) if tc.function.arguments else {}

                    for tool in self.tools:
                        current_tool_name = getattr(tool, 'name', type(tool).__name__)
                        if current_tool_name == tool_name:
                            try:
                                from google.adk.models import LlmRequest
                                class DummyToolContext:
                                    def __init__(self):
                                        self.llm_request = LlmRequest(contents=[])

                                tool_context = DummyToolContext()
                                result = await tool.run_async(args=args, tool_context=tool_context)
                                tool_results.append({
                                    "tool_call_id": tc.id,
                                    "content": str(result),
                                })
                                print(f"    âœ… Tool {tool_name} executed", flush=True)
                                break
                            except Exception as e:
                                tool_results.append({
                                    "tool_call_id": tc.id,
                                    "content": f"Error: {str(e)}",
                                })
                                break

                messages.append({
                    "role": "assistant",
                    "content": choice.content or "",
                    "tool_calls": [tc.model_dump() for tc in choice.tool_calls],
                })
                for tr in tool_results:
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tr["tool_call_id"],
                        "content": tr["content"],
                    })

            except Exception as e:
                return AgentResponse(
                    content=f"ì—ëŸ¬ ë°œìƒ: {str(e)}",
                    metadata={"error": str(e), "iteration": iteration},
                    success=False,
                    agent_name=self.name
                )

        return AgentResponse(
            content="ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ë„ë‹¬",
            metadata={"iterations": max_iterations},
            success=False,
            agent_name=self.name
        )


class MultiAgentOrchestrator:
    """ë©€í‹° ì—ì´ì „íŠ¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° - ë™ì  ì—ì´ì „íŠ¸ ê´€ë¦¬"""

    def __init__(self):
        self.servers: Dict[str, MCPServerConnection] = {}
        self.all_tools: List[BaseTool] = []
        self.llm_client: Optional[LLMClient] = None

        self.question_agent: Optional[QuestionUnderstandingAgent] = None
        self.specialist_agents: Dict[str, ToolBasedAgent] = {}  # MCP ì„œë²„ë³„ ì—ì´ì „íŠ¸

    def _normalize_url(self, url: str) -> str:
        """URL ì •ê·œí™”"""
        return url if url.endswith("/") else url + "/"

    def _prefix_tool(self, tool: BaseTool, prefix: str) -> BaseTool:
        """ë„êµ¬ ì´ë¦„ì— prefix ì¶”ê°€"""
        class PrefixedTool(tool.__class__):
            @property
            def name(self):
                original = getattr(super(), "name", getattr(tool, "name", type(tool).__name__))
                return f"{prefix}__{original}"

        wrapped = PrefixedTool.__new__(PrefixedTool)
        wrapped.__dict__ = tool.__dict__.copy()
        return wrapped

    async def connect_mcp_server(self, server_name: str, base_url: str, auth_bearer: str = "") -> MCPServerConnection:
        """MCP ì„œë²„ ì—°ê²° ë° ì—ì´ì „íŠ¸ ìë™ ìƒì„±"""
        base_url = self._normalize_url(base_url)
        headers = {}
        if auth_bearer:
            headers["Authorization"] = f"Bearer {auth_bearer}"
        try:
            conn_params = StreamableHTTPConnectionParams(
                url=base_url,
                headers=headers if headers else None,
                timeout=10.0,
                sse_read_timeout=300.0,
            )
            toolset = McpToolset(connection_params=conn_params)
            tools = await toolset.get_tools()
            connection = MCPServerConnection(
                name=server_name,
                toolset=toolset
            )
            self.servers[server_name] = connection

            # ë„êµ¬ì— prefix ì¶”ê°€
            prefixed_tools = []
            for tool in tools:
                prefixed_tool = self._prefix_tool(tool, server_name)
                prefixed_tools.append(prefixed_tool)
                self.all_tools.append(prefixed_tool)

            # ì—ì´ì „íŠ¸ ìë™ ìƒì„± (ì´ˆê¸°í™” í›„)
            # initialize_agentsì—ì„œ ì²˜ë¦¬

            return connection
        except Exception as e:
            error_msg = str(e)
            print(f"\n[Error] Failed to connect to {server_name}: {error_msg}")
            print(f" URL: {base_url}")
            if server_name in self.servers:
                del self.servers[server_name]
            raise RuntimeError(f"Failed to connect to MCP server {server_name} at {base_url}: {error_msg}") from e

    def initialize_agents(self):
        """ê°œë³„ íŠ¹í™” ì—ì´ì „íŠ¸ë“¤ì„ ë™ì ìœ¼ë¡œ ì´ˆê¸°í™”"""

        # LLM ì œê³µì ì„ íƒ (í™˜ê²½ë³€ìˆ˜ì—ì„œ ì½ê¸°, ê¸°ë³¸ê°’: azure)
        llm_provider = os.environ.get("LLM_PROVIDER", "azure").lower()

        # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        if llm_provider == "azure":
            self.llm_client = LLMClient(
                provider="azure",
                api_key=os.environ["AZURE_OPENAI_API_KEY"],
                api_version=os.environ["AZURE_OPENAI_API_VERSION"],
                azure_endpoint=os.environ["AZURE_OPENAI_ENDPOINT"],
                deployment=os.environ["AZURE_OPENAI_DEPLOYMENT"],
            )
        elif llm_provider == "openai":
            self.llm_client = LLMClient(
                provider="openai",
                api_key=os.environ.get("OPENAI_API_KEY"),
                base_url=os.environ.get("OPENAI_BASE_URL"),  # ì„ íƒì 
                model=os.environ.get("OPENAI_MODEL", "gpt-4"),
            )
        elif llm_provider == "vllm":
            self.llm_client = LLMClient(
                provider="vllm",
                api_key=os.environ.get("VLLM_API_KEY", "EMPTY"),
                base_url=os.environ["VLLM_BASE_URL"],
                model=os.environ.get("VLLM_MODEL", "meta-llama/Llama-2-7b-chat-hf"),
            )
        elif llm_provider == "google":
            self.llm_client = LLMClient(
                provider="google",
                api_key=os.environ.get("GEMINI_API_KEY"),
                base_url=os.environ.get("GEMINI_BASE_URL"),  # OpenAI í˜¸í™˜ ëª¨ë“œìš©
                model=os.environ.get("GEMINI_MODEL", "gemini-1.5-pro"),
            )
        elif llm_provider == "anthropic":
            self.llm_client = LLMClient(
                provider="anthropic",
                api_key=os.environ.get("ANTHROPIC_API_KEY"),
                base_url=os.environ.get("ANTHROPIC_BASE_URL"),  # OpenAI í˜¸í™˜ ëª¨ë“œìš©
                model=os.environ.get("ANTHROPIC_MODEL", "claude-3-5-sonnet-20241022"),
            )
        elif llm_provider == "xai":
            self.llm_client = LLMClient(
                provider="xai",
                api_key=os.environ.get("XAI_API_KEY"),
                base_url=os.environ.get("XAI_BASE_URL"),
                model=os.environ.get("XAI_MODEL", "grok-beta"),
            )
        else:
            raise ValueError(f"Unsupported LLM_PROVIDER: {llm_provider}. Use 'azure', 'openai', 'vllm', 'google', 'anthropic', or 'xai'")

        # ê° ì„œë²„ë³„ ë„êµ¬ ì •ë³´ ìˆ˜ì§‘
        agent_tools_info = {}
        for server_name in self.servers.keys():
            server_tools = [tool for tool in self.all_tools if getattr(tool, 'name', '').startswith(f'{server_name}__')]
            tool_names = [getattr(tool, 'name', '').replace(f'{server_name}__', '') for tool in server_tools]
            agent_tools_info[server_name] = tool_names

        # Agent A ì´ˆê¸°í™” (ë¼ìš°íŒ… ì—ì´ì „íŠ¸) - ë„êµ¬ ì •ë³´ í¬í•¨
        available_agents = list(self.servers.keys())
        self.question_agent = QuestionUnderstandingAgent(self.llm_client, available_agents, agent_tools_info)

        # ê° MCP ì„œë²„ë³„ë¡œ ì—ì´ì „íŠ¸ ìë™ ìƒì„±
        print(f"\nâœ… ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ:")
        print(f"   â€¢ {self.question_agent.name}: {self.question_agent.role}")

        for server_name in self.servers.keys():
            # í•´ë‹¹ ì„œë²„ì˜ ë„êµ¬ë§Œ í•„í„°ë§
            server_tools = [tool for tool in self.all_tools if getattr(tool, 'name', '').startswith(f'{server_name}__')]

            # ì—ì´ì „íŠ¸ ìƒì„±
            agent = ToolBasedAgent(
                name=f"{server_name.upper()}Agent",
                role=f"{server_name} ì „ë¬¸ ì„œë¹„ìŠ¤",
                llm_client=self.llm_client,
                tools=server_tools
            )
            self.specialist_agents[server_name] = agent
            print(f"   â€¢ {agent.name}: {agent.role} (ë„êµ¬ {len(server_tools)}ê°œ)")
            # ë„êµ¬ ëª©ë¡ ì¶œë ¥
            for tool in server_tools:
                tool_name = getattr(tool, 'name', '')
                print(f"      - {tool_name}")
                # tool_description = getattr(tool, 'description', '')
                # print(f"      - {tool_name} {tool_description}")

    async def close_all_servers(self):
        """ëª¨ë“  MCP ì„œë²„ ì—°ê²° ì¢…ë£Œ"""
        for server_name, server in list(self.servers.items()):
            try:
                if server.toolset:
                    await asyncio.wait_for(server.toolset.close(), timeout=5.0)
                    print(f"âœ“ Closed {server_name}")
            except asyncio.TimeoutError:
                print(f"âš ï¸  Timeout closing server {server_name}")
            except asyncio.CancelledError:
                print(f"âš ï¸  Cancelled while closing server {server_name}")
            except Exception as e:
                print(f"âš ï¸  Error closing server {server_name}: {e}")
        self.servers.clear()
        self.all_tools.clear()
        self.specialist_agents.clear()


async def initialize_multi_agent() -> MultiAgentOrchestrator:
    """ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™” - ë™ì  MCP ì„œë²„ ì—°ê²°"""
    orchestrator = MultiAgentOrchestrator()
    connected_servers = []

    try:
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ MCP ì„œë²„ ëª©ë¡ ë™ì  ë¡œë“œ
        server_index = 1
        while True:
            url_key = f"MCP_SERVER_{server_index}_URL"
            bearer_key = f"MCP_SERVER_{server_index}_AUTH_BEARER"
            name_key = f"MCP_SERVER_{server_index}_NAME"

            server_url = os.environ.get(url_key, "")
            if not server_url:
                break

            server_bearer = os.environ.get(bearer_key, "")
            server_name = os.environ.get(name_key, f"mcp{server_index}")

            print(f"Connecting to MCP Server '{server_name}': {server_url}")
            try:
                await orchestrator.connect_mcp_server(server_name, server_url, server_bearer)
                print(f"âœ“ Connected to {server_name}")
                connected_servers.append(server_name)
            except Exception as e:
                print(f"âœ— Failed to connect to {server_name}: {e}")

            server_index += 1

        if not connected_servers:
            raise RuntimeError("No MCP servers connected. Check your .env configuration.")

        print(f"\nâœ… Connected to {len(connected_servers)} MCP server(s): {', '.join(connected_servers)}")
        print("\nInitializing Multi-Agent System...")
        orchestrator.initialize_agents()
        print(f"âœ“ Multi-Agent System initialized with {len(orchestrator.all_tools)} total tools\n")

        return orchestrator

    except Exception as e:
        raise RuntimeError(f"Failed to initialize multi-agent system: {e}") from e


async def run_multi_agent_conversation(orchestrator: MultiAgentOrchestrator, user_query: str) -> str:
    """ë©€í‹° ì—ì´ì „íŠ¸ ëŒ€í™” ì‹¤í–‰"""

    print(f"\n{'='*70}")
    print(f"ğŸ¤– Multi-Agent Processing Pipeline")
    print(f"{'='*70}\n")

    # Step 1: Agent A - ì§ˆë¬¸ ë¶„ì„
    print(f"ğŸ“‹ [Step 1] Agent A: ì§ˆë¬¸ ë¶„ì„ ë° ë¼ìš°íŒ…")
    print(f"{'â”€'*70}")

    step1_start = time.time()
    question_response = await orchestrator.question_agent.process(user_query)
    step1_time = time.time() - step1_start

    if not question_response.success:
        print(f"âŒ ì§ˆë¬¸ ë¶„ì„ ì‹¤íŒ¨: {question_response.content}")
        return question_response.content

    print(f"âœ… ë¶„ì„ ì™„ë£Œ ({step1_time:.2f}ì´ˆ)")
    print(f"\n{question_response.content}\n")

    # JSON íŒŒì‹±
    try:
        content = question_response.content
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].split("```")[0].strip()

        analysis = json.loads(content)
        execution_order = analysis.get("execution_order", [])
        question_type = analysis.get("question_type", "general")
        queries = analysis.get("queries", {})
        dependencies = analysis.get("dependencies", {})

        print(f"ğŸ¯ íŒë‹¨ ê²°ê³¼:")
        print(f"   ì§ˆë¬¸ ìœ í˜•: {question_type}")

        # execution_orderë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥ (ë³‘ë ¬ì€ ê´„í˜¸ë¡œ, ìˆœì°¨ëŠ” í™”ì‚´í‘œë¡œ)
        if execution_order:
            execution_plan_str = " â†’ ".join([
                f"({', '.join(group)})" if len(group) > 1 else group[0]
                for group in execution_order
            ])
            print(f"   ì‹¤í–‰ ìˆœì„œ: {execution_plan_str}")
        else:
            print(f"   ì‹¤í–‰ ìˆœì„œ: none")

        # ê° ì—ì´ì „íŠ¸ë³„ ì§ˆë¬¸ ì¶œë ¥
        if queries:
            flat_order = [agent for group in execution_order for agent in group]
            for agent in flat_order:
                if agent in queries:
                    print(f"   - {agent}: {queries[agent]}")
                    if agent in dependencies:
                        print(f"      â””â”€ ì˜ì¡´ì„±: {dependencies[agent]}")
        print()

    except json.JSONDecodeError:
        print(f"âš ï¸  JSON íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ ì²˜ë¦¬ë¡œ ì§„í–‰\n")
        execution_order = []
        queries = {}
        dependencies = {}

    # Step 2: ì „ë¬¸ ì—ì´ì „íŠ¸ë“¤ ìˆœì°¨ ì‹¤í–‰
    if not execution_order:
        # ì¼ë°˜ ì§ˆë¬¸ - Agent Aê°€ ì§ì ‘ ë‹µë³€
        print(f"ğŸ’¬ [Final Answer] Agent A ì§ì ‘ ë‹µë³€")
        print(f"{'â”€'*70}\n")

        try:
            messages = [
                {"role": "system", "content": """ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”."""},
                {"role": "user", "content": user_query}
            ]

            stream_start = time.time()
            stream_response = orchestrator.llm_client.chat_completion(messages, stream=True)

            collected_content = ""
            for chunk in stream_response:
                if chunk.choices and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta
                    if hasattr(delta, 'content') and delta.content:
                        print(delta.content, end="", flush=True)
                        collected_content += delta.content

            stream_time = time.time() - stream_start
            print(f"\n\nâ±ï¸  ë‹µë³€ ìƒì„± ì‹œê°„: {stream_time:.2f}ì´ˆ")

            return collected_content

        except Exception as e:
            print(f"\nâš ï¸  ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return question_response.content

    agent_results = {}
    previous_results = []
    step_num = 2

    # execution_orderë¥¼ ê·¸ë£¹ë³„ë¡œ ì‹¤í–‰ (ê° ê·¸ë£¹ ë‚´ë¶€ëŠ” ë³‘ë ¬, ê·¸ë£¹ ê°„ì€ ìˆœì°¨)
    for group_idx, agent_group in enumerate(execution_order):
        if not agent_group:
            continue

        # ë‹¨ì¼ ì—ì´ì „íŠ¸ (ìˆœì°¨ ì‹¤í–‰)
        if len(agent_group) == 1:
            agent_name = agent_group[0]
            if agent_name not in orchestrator.specialist_agents:
                print(f"âš ï¸  ì—ì´ì „íŠ¸ '{agent_name}' not found, skipping...")
                continue

            specialist_agent = orchestrator.specialist_agents[agent_name]
            print(f"ğŸ”§ [Step {step_num}] {specialist_agent.name} ì²˜ë¦¬ (ìˆœì°¨)")
            print(f"{'â”€'*70}")

            query = queries.get(agent_name, user_query)
            dependency = dependencies.get(agent_name, "")

            print(f"ì§ˆë¬¸: {query}")
            if dependency and previous_results:
                print(f"ì˜ì¡´ì„±: {dependency}")
            print()

            step_start = time.time()
            context = {
                "original_query": user_query,
                "analysis": question_response.content,
                "structured_query": query,
            }
            if previous_results:
                context["previous_agent_results"] = previous_results
                if dependency:
                    context["dependency_instruction"] = dependency

            response = await specialist_agent.process_with_tools(query, context)
            step_time = time.time() - step_start

            if response.success:
                print(f"\nâœ… ì²˜ë¦¬ ì™„ë£Œ ({step_time:.2f}ì´ˆ)\n")
                result_info = {
                    "agent": specialist_agent.name,
                    "agent_name": agent_name,
                    "query": query,
                    "response": response.content,
                    "time": step_time
                }
                agent_results[agent_name] = result_info
                previous_results.append(result_info)
            else:
                print(f"\nâŒ ì²˜ë¦¬ ì‹¤íŒ¨: {response.content}\n")

        # ì—¬ëŸ¬ ì—ì´ì „íŠ¸ (ë³‘ë ¬ ì‹¤í–‰)
        else:
            print(f"ğŸš€ [Step {step_num}] {len(agent_group)}ê°œ ì—ì´ì „íŠ¸ ë³‘ë ¬ ì²˜ë¦¬: {', '.join(agent_group)}")
            print(f"{'â”€'*70}")

            tasks = []
            task_agent_names = []

            for agent_name in agent_group:
                if agent_name not in orchestrator.specialist_agents:
                    print(f"âš ï¸  ì—ì´ì „íŠ¸ '{agent_name}' not found, skipping...")
                    continue

                specialist_agent = orchestrator.specialist_agents[agent_name]
                query = queries.get(agent_name, user_query)
                dependency = dependencies.get(agent_name, "")

                print(f"  - {specialist_agent.name}")
                print(f"    ì§ˆë¬¸: {query}")
                if dependency and previous_results:
                    print(f"    ì˜ì¡´ì„±: {dependency}")

                context = {
                    "original_query": user_query,
                    "analysis": question_response.content,
                    "structured_query": query,
                }
                if previous_results:
                    context["previous_agent_results"] = previous_results
                    if dependency:
                        context["dependency_instruction"] = dependency

                tasks.append(specialist_agent.process_with_tools(query, context))
                task_agent_names.append(agent_name)

            if tasks:
                print()
                step_start = time.time()
                parallel_responses = await asyncio.gather(*tasks)
                step_time = time.time() - step_start

                print(f"âœ… ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ ({step_time:.2f}ì´ˆ)\n")

                # ë³‘ë ¬ ì²˜ë¦¬ ê²°ê³¼ë¥¼ ìˆœì„œëŒ€ë¡œ ì €ì¥
                for agent_name, response in zip(task_agent_names, parallel_responses):
                    if response.success:
                        result_info = {
                            "agent": response.agent_name,
                            "agent_name": agent_name,
                            "query": queries.get(agent_name, user_query),
                            "response": response.content,
                            "time": step_time  # ë³‘ë ¬ ì‹¤í–‰ì€ ì „ì²´ ì‹œê°„ ì‚¬ìš©
                        }
                        agent_results[agent_name] = result_info
                        previous_results.append(result_info)
                        print(f"  âœ“ {response.agent_name}: ì„±ê³µ")
                    else:
                        print(f"  âœ— {response.agent_name}: ì‹¤íŒ¨ - {response.content}")
                print()

        step_num += 1

    if not agent_results:
        return question_response.content

    # Step 3: Agent A - ê²°ê³¼ í†µí•©
    print(f"ğŸ”„ [Step {step_num}] Agent A: ê²°ê³¼ í†µí•© ë° ìµœì¢… ë‹µë³€ ìƒì„±")
    print(f"{'â”€'*70}\n")

    try:
        expert_answers = ""
        # execution_orderë¥¼ flatí•˜ê²Œ ë§Œë“¤ì–´ì„œ ìˆœì„œëŒ€ë¡œ ì¶œë ¥
        flat_execution_order = [agent for group in execution_order for agent in group]
        for i, agent_name in enumerate(flat_execution_order, 1):
            if agent_name in agent_results:
                result = agent_results[agent_name]
                expert_answers += f"\n\n[{i}ë‹¨ê³„: {result['agent']}ì˜ ë‹µë³€]\nì§ˆë¬¸: {result['query']}\në‹µë³€: {result['response']}"

        messages = [
            {"role": "system", "content": """ë‹¹ì‹ ì€ ì—¬ëŸ¬ ì „ë¬¸ê°€ì˜ ë‹µë³€ì„ í†µí•©í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ì œê³µí•˜ëŠ” ì½”ë””ë„¤ì´í„°ì…ë‹ˆë‹¤.
ê° ì „ë¬¸ê°€ì˜ ë‹µë³€ì„ ì‹¤í–‰ ìˆœì„œëŒ€ë¡œ ì¢…í•©í•˜ì—¬ ëª…í™•í•˜ê³  ì²´ê³„ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."""},
            {"role": "user", "content": f"""ì›ë³¸ ì§ˆë¬¸: {user_query}

ì „ë¬¸ê°€ ë‹µë³€ë“¤ (ì‹¤í–‰ ìˆœì„œ):
{expert_answers}

ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”."""}
        ]

        stream_start = time.time()
        stream_response = orchestrator.llm_client.chat_completion(messages, stream=True)

        collected_content = ""
        for chunk in stream_response:
            if chunk.choices and len(chunk.choices) > 0:
                delta = chunk.choices[0].delta
                if hasattr(delta, 'content') and delta.content:
                    print(delta.content, end="", flush=True)
                    collected_content += delta.content

        stream_time = time.time() - stream_start
        print(f"\n\nâ±ï¸  í†µí•© ë‹µë³€ ìƒì„± ì‹œê°„: {stream_time:.2f}ì´ˆ")

        return collected_content

    except Exception as e:
        print(f"\nâš ï¸  í†µí•© ì‹¤íŒ¨: {e}")
        return "\n\n".join([f"[{r['agent']}]\n{r['response']}" for r in agent_results.values()])


async def main():
    print("="*70)
    print("ğŸ¤– Dynamic Multi-Agent System")
    print("="*70)

    orchestrator = None
    try:
        orchestrator = await initialize_multi_agent()
        print("\nâœ… Multi-Agent System is ready!")
        print("Type 'quit' or 'exit' to stop.\n")

        while True:
            q = input("\nğŸ§‘ You> ").strip()
            if q.lower() in {"quit", "exit", "q"}:
                break
            if not q:
                continue

            try:
                conversation_start = time.time()
                ans = await run_multi_agent_conversation(orchestrator, q)
                total_time = time.time() - conversation_start
                print(f"\nâ±ï¸  ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ")
            except Exception as e:
                print(f"\nâŒ [Error] {e}")
                import traceback
                traceback.print_exc()

    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ Interrupted by user.")
    except Exception as e:
        print(f"\nâŒ [Fatal Error] {e}")
        import traceback
        traceback.print_exc()
    finally:
        if orchestrator:
            print("\nğŸ”Œ Closing all MCP server connections...")
            try:
                await orchestrator.close_all_servers()
                print("âœ“ All connections closed.")
            except Exception as e:
                print(f"âš ï¸  Error during cleanup: {e}")


if __name__ == "__main__":
    asyncio.run(main())

